{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94a2592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "from utils.components import *\n",
    "from einops.layers.torch import EinMix\n",
    "from utils.masking import *\n",
    "from utils.einmask import EinMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0732b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import *\n",
    "from omegaconf import OmegaConf\n",
    "from dataclasses import replace, dataclass\n",
    "from math import prod\n",
    "\n",
    "from utils.components import *\n",
    "from utils.config import *\n",
    "from utils.random_fields import RandomField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e54d1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = WorldConfig({'v': 6, 't':36,'h': 64, 'w':120}, {'vv':6, 'tt':6,'hh':4, 'ww':4}, batch_size=4)\n",
    "objective = ObjectiveConfig()\n",
    "network = NetworkConfig(512, 8, 256, dim_noise=32, num_tails=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a57e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d600ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "\n",
    "from einops.layers.torch import EinMix\n",
    "\n",
    "from utils.components import *\n",
    "from utils.config import *\n",
    "\n",
    "class EinMask(torch.nn.Module):\n",
    "    def __init__(self, network: NetworkConfig, world: WorldConfig):\n",
    "        super().__init__()\n",
    "        # store configs\n",
    "        self.network = network\n",
    "        self.world = world\n",
    "\n",
    "        # I/O\n",
    "        self.to_tokens = EinMix(\n",
    "            pattern=f\"{world.field_pattern} -> b {world.flat_token_pattern} d\", \n",
    "            weight_shape=f'{world.patch_pattern} v d', \n",
    "            d = network.dim, \n",
    "            **world.patch_sizes, **world.token_sizes\n",
    "            )\n",
    "        \n",
    "        self.to_fields = EinMix(\n",
    "            pattern=f\"b {world.flat_token_pattern} d -> {world.field_pattern} k\", \n",
    "            weight_shape=f'd v {world.patch_pattern} k', \n",
    "            d = network.dim, \n",
    "            k = default(network.num_tails, 1), \n",
    "            **world.patch_sizes, **world.token_sizes \n",
    "            )\n",
    "                \n",
    "        # noise mapping\n",
    "        if default(network.num_tails, 1) > 1:\n",
    "            self.to_noise = None\n",
    "            self.noise_generator = None\n",
    "        elif exists(network.dim_noise):\n",
    "            self.to_noise = GatedFFN(network.dim_noise)\n",
    "            self.noise_generator = None\n",
    "        else:\n",
    "            self.noise_generator = RandomField(network.dim, world, has_ffn=False)\n",
    "            self.to_noise = None\n",
    "        \n",
    "        # positional embeddings\n",
    "        self.src_positions = ContinuousPositionalEmbedding(\n",
    "            dim_per_coord=network.dim_coords, \n",
    "            wavelengths=[(1, 2 * k) for k in world.token_shape],\n",
    "            model_dim=network.dim\n",
    "        )\n",
    "\n",
    "        self.tgt_positions = ContinuousPositionalEmbedding(\n",
    "            dim_per_coord=network.dim_coords, \n",
    "            wavelengths=[(1, 2 * k) for k in world.token_shape],\n",
    "            model_dim=network.dim\n",
    "        )\n",
    "        \n",
    "        # pre-computed coordinates\n",
    "        self.register_buffer('indices', torch.arange(world.num_tokens))\n",
    "        self.register_buffer(\"coordinates\", torch.stack(\n",
    "            torch.unravel_index(indices = self.indices, shape = world.token_shape), \n",
    "            dim = -1)\n",
    "            )        \n",
    "        \n",
    "        # learnable latents\n",
    "        self.latents = torch.nn.Embedding(network.num_latents, network.dim)\n",
    "\n",
    "        # latent transformer\n",
    "        self.encoder = torch.nn.ModuleList([\n",
    "            TransformerBlock(network.dim, drop_path=network.drop_path, dim_ctx=network.dim_noise)\n",
    "            for _ in range(network.num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoder = torch.nn.ModuleList([\n",
    "            TransformerBlock(network.dim, drop_path=network.drop_path, dim_ctx=network.dim_noise)\n",
    "            for _ in range(network.num_compute_blocks)\n",
    "        ])\n",
    "\n",
    "        # Weight initialization\n",
    "        self.apply(self.base_init)\n",
    "\n",
    "    @staticmethod\n",
    "    def base_init(m: torch.nn.Module):\n",
    "        # linear\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = 0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        # embedding\n",
    "        if isinstance(m, torch.nn.Embedding):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = 0.02)\n",
    "        # einmix\n",
    "        if isinstance(m, EinMix):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = 0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.trunc_normal_(m.bias, std = 0.02)\n",
    "        # conditional layer norm\n",
    "        if isinstance(m, ConditionalLayerNorm):\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "            if m.weight is not None: # CLN weight close to 0\n",
    "                torch.nn.init.trunc_normal_(m.weight, std = 1e-7)\n",
    "    \n",
    "    def forward(self, \n",
    "                fields: torch.FloatTensor, \n",
    "                srcs: List[torch.LongTensor] | torch.LongTensor, \n",
    "                tgts: List[torch.LongTensor] | torch.LongTensor = None,\n",
    "                members: Optional[int] = None, \n",
    "                rng: Optional[torch.Generator] = None\n",
    "                ) -> torch.FloatTensor:\n",
    "        B = fields.size(0)\n",
    "        D = self.network.dim\n",
    "        K = default(self.network.num_tails, 1)\n",
    "        E = default(members, 1)\n",
    "        tgts = default(tgts, self.indices.expand(B, -1))\n",
    "\n",
    "        # expand to ensemble form\n",
    "        fields = einops.repeat(fields, \"b ... -> (b e) ...\", e = E, b = B)\n",
    "        latents = einops.repeat(self.latents.weight, '... -> (b e) ...', b = B, e = E)\n",
    "        coo = einops.repeat(self.coordinates, '... -> (b e) ...', b = B, e = E)\n",
    "        src_idx = einops.repeat(srcs, 'b ... -> (b e) ... d', d = D, e = E, b = B)\n",
    "        tgt_idx = einops.repeat(tgts, 'b ... -> (b e) ... d', d = D, e = E, b = B)\n",
    "        \n",
    "        # embed full fields as tokens\n",
    "        tokens = self.to_tokens(fields) + self.src_positions(coo)\n",
    "\n",
    "        # gather tokens visible at this step\n",
    "        context = tokens.gather(1, src_idx)\n",
    "\n",
    "        # prepare queries\n",
    "        queries = self.tgt_positions(coo).gather(1, tgt_idx)\n",
    "\n",
    "        # maybe create functional noise\n",
    "        if exists(self.to_noise):\n",
    "            ctx = torch.randn(B * E, 1, self.network.dim_noise, generator = rng, device = fields.device)\n",
    "            ctx = self.to_noise(ctx)\n",
    "        elif exists(self.noise_generator):\n",
    "            context, noise = self.noise_generator(context, rng = rng)\n",
    "            queries = queries + noise.gather(1, tgt_idx)\n",
    "            context = context + noise.gather(1, src_idx)\n",
    "            ctx = None\n",
    "        else:\n",
    "            ctx = None\n",
    "\n",
    "        # map context to latents\n",
    "        for read in self.encoder:\n",
    "            latents = read(q = latents, kv = torch.cat([latents, context], dim = 1), ctx = ctx)\n",
    "\n",
    "        # map latents to queries\n",
    "        for write in self.decoder:\n",
    "            queries = write(q = queries, kv = torch.cat([queries, latents], dim = 1), ctx = ctx)\n",
    "\n",
    "        # scatter tokens predicted at this step\n",
    "        tokens = tokens.scatter(1, tgt_idx, queries)\n",
    "\n",
    "        # map all tokens back to fields\n",
    "        fields = self.to_fields(tokens)\n",
    "        \n",
    "        # rearrange to ensemble form\n",
    "        fields = einops.rearrange(fields, \"(b e) ... k -> b ... (e k)\", e = E, b = B, k = K)\n",
    "        return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a79ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396152ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
