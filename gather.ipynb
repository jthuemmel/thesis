{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a2592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "from utils.components import *\n",
    "from utils.model import MaskedPredictor\n",
    "from einops.layers.torch import EinMix\n",
    "from utils.masking import KumaraswamyMasking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0732b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import *\n",
    "from omegaconf import OmegaConf\n",
    "from dataclasses import replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7dcd6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaggedPredictor(torch.nn.Module):\n",
    "    def __init__(self, model, world):\n",
    "        super().__init__()\n",
    "        # Attributes\n",
    "        self.dim_noise = model.dim_noise\n",
    "        \n",
    "        # Learnable tokens\n",
    "        self.positions = torch.nn.Embedding(world.num_tokens, model.dim)\n",
    "        self.queries = torch.nn.Embedding(world.num_tokens, model.dim)\n",
    "        self.latents = torch.nn.Embedding(model.num_latents, model.dim)\n",
    "        self.masks = torch.nn.Embedding(1, model.dim)\n",
    "        \n",
    "        # Projections for latents and noise\n",
    "        self.proj_noise = torch.nn.Sequential(\n",
    "            GatedFFN(model.dim_noise), torch.nn.LayerNorm(model.dim_noise)\n",
    "        ) if exists(model.dim_noise) else torch.nn.Identity()\n",
    "        self.proj_latents = SelfConditioning(dim=model.dim)\n",
    "        \n",
    "        # Per-variable I/O projections\n",
    "        self.norm_in = torch.nn.LayerNorm(model.dim)\n",
    "        self.proj_in = EinMix(\n",
    "            pattern = f'{world.flatland_pattern} -> b {world.flat_token_pattern} d',\n",
    "            weight_shape = f'v {world.patch_pattern} d', \n",
    "            bias_shape = 'v d',\n",
    "            d = model.dim, **world.patch_sizes, **world.token_sizes\n",
    "            )\n",
    "        \n",
    "        self.proj_out = EinMix(\n",
    "            pattern = f'b {world.flat_token_pattern} d -> {world.flatland_pattern}',\n",
    "            weight_shape = f'v {world.patch_pattern} d',\n",
    "            bias_shape = f'v {world.patch_pattern}',\n",
    "            d = model.dim, **world.patch_sizes, **world.token_sizes\n",
    "            )\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = InterfaceBlock(\n",
    "            dim=model.dim, \n",
    "            num_blocks=model.num_layers,\n",
    "            dim_ctx=model.dim_noise, \n",
    "            dim_heads=model.dim_heads, \n",
    "            write_has_skip=False, \n",
    "            use_checkpoint=model.use_checkpoint\n",
    "        )\n",
    "        \n",
    "        # Weight initialization\n",
    "        self.apply(self.base_init)\n",
    "    \n",
    "    @staticmethod\n",
    "    def base_init(m):\n",
    "        '''Explicit weight initialization for all components'''\n",
    "        # linear\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = get_weight_std(m.weight))\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        # embedding\n",
    "        if isinstance(m, torch.nn.Embedding):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = get_weight_std(m.weight))\n",
    "        # einmix\n",
    "        if isinstance(m, EinMix):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = get_weight_std(m.weight))\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        # layer norm\n",
    "        if isinstance(m, torch.nn.LayerNorm):\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "            if m.weight is not None:\n",
    "                torch.nn.init.ones_(m.weight)\n",
    "        # conditional layer norm\n",
    "        if isinstance(m, ConditionalLayerNorm):\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "            if m.weight is not None: # CLN weight close to 0\n",
    "                torch.nn.init.trunc_normal_(m.weight, std = 1e-7)\n",
    "        # self conditioning\n",
    "        if isinstance(m, SelfConditioning):\n",
    "            torch.nn.init.trunc_normal_(m.scale, std = 1e-7)\n",
    "\n",
    "    def step(self, \n",
    "             tokens: torch.FloatTensor, \n",
    "             mask: torch.BoolTensor, \n",
    "             latents: torch.FloatTensor = None,\n",
    "             noise: torch.FloatTensor = None\n",
    "             ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        # input projection and positional embeddings\n",
    "        x = self.proj_in(tokens) + self.positions.weight\n",
    "        x = self.norm_in(x) \n",
    "\n",
    "        # # self-condition latents:\n",
    "        z = self.latents.weight.expand(tokens.size(0), -1, -1)\n",
    "        z = self.proj_latents(initial = z, previous = latents)\n",
    "\n",
    "        # shared noise projection\n",
    "        if exists(noise): \n",
    "            noise = self.proj_noise(noise)\n",
    "\n",
    "        # mask into jagged tensor\n",
    "        x = torch.nested.as_nested_tensor([x[i, m] for i, m in enumerate(mask)], layout=torch.jagged)\n",
    "        z = torch.nested.as_nested_tensor(z, layout=torch.jagged)\n",
    "        q = torch.nested.as_nested_tensor(self.queries.weight.expand(tokens.size(0), -1, -1), layout=torch.jagged)\n",
    "        \n",
    "        # perceiver\n",
    "        q, z = self.transformer(x = x, z = z, query = q, ctx = noise)\n",
    "\n",
    "        # back to dense layout\n",
    "        q = torch.stack(q.unbind())\n",
    "        z = torch.stack(z.unbind())\n",
    "\n",
    "        # output projection\n",
    "        q = self.proj_out(q)\n",
    "        return q, z\n",
    "    \n",
    "    def forward(self, \n",
    "                tokens: torch.FloatTensor, \n",
    "                masks: torch.BoolTensor, \n",
    "                E: int, \n",
    "                rng: torch.Generator = None\n",
    "                ) -> tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        '''\n",
    "        Args:\n",
    "            tokens: (B, N, C_in) Tensor of input tokens\n",
    "            masks: (S, B, N) BoolTensor of masks for S steps\n",
    "            E: Number of ensemble members\n",
    "            rng: torch.Generator for random number generation\n",
    "        Returns:\n",
    "            x: (B, N, C_out, E) Tensor of predicted tokens\n",
    "            z: (B, L, D, E) Tensor of latent variables after processing\n",
    "        '''\n",
    "        S, B, N = masks.size()\n",
    "        \n",
    "        # parallelise ensemble processing\n",
    "        fs = torch.randn((S, B * E, 1, self.dim_noise), device = tokens.device, generator = rng)\n",
    "        xs = einops.repeat(tokens, \"b n c -> (b e) n c\", e = E, b = B, n = N)\n",
    "        ms = einops.repeat(masks, 's b n -> s (b e) n', e = E, b = B, n = N, s = S)\n",
    "\n",
    "        # iterate\n",
    "        zs = None\n",
    "        for s in range(S):\n",
    "            xs, zs = self.step(tokens = xs, mask = ms[s], latents = zs, noise = fs[s])\n",
    "            # detach gradients unless it is the last step\n",
    "            if s < S - 1:\n",
    "                xs, zs = (xs.detach(), zs.detach())\n",
    "        \n",
    "        # rearrange to ensemble form\n",
    "        xs = einops.rearrange(xs, \"(b e) n c -> b n c e\", e = E, b = B, n = N)\n",
    "        zs = einops.rearrange(zs, \"(b e) l d -> b l d e\", e = E, b = B)\n",
    "        return xs, zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e54d1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = WorldConfig({'v': 6, 't':36,'h': 64, 'w':120}, {'vv':6, 'tt':6,'hh':4, 'ww':4}, batch_size=4)\n",
    "objective = ObjectiveConfig()\n",
    "network = NetworkConfig(512, 8, 256, dim_noise=32, use_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5936a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "ks = KumaraswamyMasking(world=world, objective=objective).to(device)\n",
    "model = JaggedPredictor(network, world).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "601684b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn((world.batch_size, world.num_tokens, world.dim_tokens), device = device)\n",
    "mask, weight = ks((world.batch_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f70043fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test, _ = model.forward(data, mask[None,...], E = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de474c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7628093f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
