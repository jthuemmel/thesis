{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94a2592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "from utils.components import *\n",
    "from einops.layers.torch import EinMix\n",
    "from utils.masking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0732b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import *\n",
    "from omegaconf import OmegaConf\n",
    "from dataclasses import replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e54d1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = WorldConfig({'v': 6, 't':36,'h': 64, 'w':120}, {'vv':6, 'tt':6,'hh':4, 'ww':4}, batch_size=4)\n",
    "objective = ObjectiveConfig()\n",
    "network = NetworkConfig(512, 8, 256, dim_noise=32, num_tails=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "68535e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "\n",
    "from einops.layers.torch import EinMix\n",
    "\n",
    "from utils.components import *\n",
    "from utils.config import *\n",
    "from utils.random_fields import RandomField\n",
    "\n",
    "class EinMask(torch.nn.Module):\n",
    "    def __init__(self, network: NetworkConfig, world: WorldConfig):\n",
    "        super().__init__()\n",
    "        # store configs\n",
    "        self.network = network\n",
    "        self.world = world\n",
    "\n",
    "        # maybe use generative noise\n",
    "        self.noise_generator = RandomField(network.dim, world) if default(network.num_tails, 1) <= 1 else None\n",
    "\n",
    "        # I/O\n",
    "        self.to_tokens = EinMix(\n",
    "            pattern=f\"{world.field_pattern} -> b {world.flat_token_pattern} d\", \n",
    "            weight_shape=f'{world.patch_pattern} v d', \n",
    "            d = network.dim, \n",
    "            **world.patch_sizes, **world.token_sizes\n",
    "            )\n",
    "        \n",
    "        self.to_fields = EinMix(\n",
    "            pattern=f\"b {world.flat_token_pattern} d -> {world.field_pattern} k\", \n",
    "            weight_shape=f'd v {world.patch_pattern} k', \n",
    "            d = network.dim, \n",
    "            k = default(network.num_tails, 1), \n",
    "            **world.patch_sizes, **world.token_sizes \n",
    "            )\n",
    "                \n",
    "        # positional embeddings\n",
    "        self.src_positions = ContinuousPositionalEmbedding(\n",
    "            dim_per_coord=network.dim_coords, \n",
    "            wavelengths=[(1, 2 * k) for k in world.token_shape],\n",
    "            model_dim=network.dim\n",
    "        )\n",
    "\n",
    "        self.tgt_positions = ContinuousPositionalEmbedding(\n",
    "            dim_per_coord=network.dim_coords, \n",
    "            wavelengths=[(1, 2 * k) for k in world.token_shape],\n",
    "            model_dim=network.dim\n",
    "        )\n",
    "        \n",
    "        # pre-computed coordinates\n",
    "        self.register_buffer('indices', torch.arange(world.num_tokens))\n",
    "        self.register_buffer(\"coordinates\", torch.stack(\n",
    "            torch.unravel_index(indices = self.indices, shape = world.token_shape), \n",
    "            dim = -1)\n",
    "            )        \n",
    "        \n",
    "        # learnable latents\n",
    "        self.latents = torch.nn.Embedding(network.num_latents, network.dim)\n",
    "\n",
    "        # latent transformer\n",
    "        self.encoder = torch.nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                dim = network.dim, \n",
    "                dim_heads = network.dim_heads, \n",
    "                dim_ctx = network.dim_noise,\n",
    "                drop_path = network.drop_path,\n",
    "                ) \n",
    "                for _ in range(network.num_layers)\n",
    "            ])\n",
    "        \n",
    "        self.decoder = TransformerBlock(\n",
    "                dim = network.dim, \n",
    "                dim_heads = network.dim_heads, \n",
    "                dim_ctx = network.dim_noise,\n",
    "                has_skip=False,\n",
    "                )\n",
    "\n",
    "        # Weight initialization\n",
    "        self.apply(self.base_init)\n",
    "    \n",
    "    @staticmethod\n",
    "    def base_init(m):\n",
    "        '''Explicit weight initialization'''\n",
    "        # linear\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = 0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        # embedding\n",
    "        if isinstance(m, torch.nn.Embedding):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = 0.02)\n",
    "        # einmix\n",
    "        if isinstance(m, EinMix):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = 0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.trunc_normal_(m.bias, std = 0.02)\n",
    "        # conditional layer norm\n",
    "        if isinstance(m, ConditionalLayerNorm):\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "            if m.weight is not None: # CLN weight close to 0\n",
    "                torch.nn.init.trunc_normal_(m.weight, std = 1e-7)\n",
    "\n",
    "    def forward(self, \n",
    "                fields: torch.FloatTensor, \n",
    "                srcs: torch.LongTensor, \n",
    "                tgts: Optional[torch.LongTensor] = None,\n",
    "                members: Optional[int] = None, \n",
    "                rng: Optional[torch.Generator] = None\n",
    "                ) -> torch.FloatTensor:\n",
    "        # define shapes\n",
    "        B = fields.size(0)\n",
    "        D = self.network.dim\n",
    "        K = default(self.network.num_tails, 1)\n",
    "        E = default(members, 1)\n",
    "        S = srcs.size(0) if srcs.ndim > 2 else 1\n",
    "        \n",
    "        # maybe expand srcs/tgts\n",
    "        if tgts is None: tgts = self.indices.expand(S, B, -1)\n",
    "        if tgts.ndim == 2: tgts = tgts.expand(S, -1, -1)\n",
    "        if srcs.ndim == 2: srcs = srcs.expand(S, -1, -1)\n",
    "\n",
    "        # expand indices and coordinates\n",
    "        src_idx = einops.repeat(srcs, 's b ... -> s (b e) ... d', d = D, e = E, b = B, s = S)\n",
    "        tgt_idx = einops.repeat(tgts, 's b ... -> s (b e) ... d', d = D, e = E, b = B, s = S)\n",
    "        src_coo = einops.repeat(self.coordinates[srcs], 's b ... -> s (b e) ...', e = E, b = B, s = S)\n",
    "        tgt_coo = einops.repeat(self.coordinates[tgts], 's b ... -> s (b e) ...', e = E, b = B, s = S)\n",
    "\n",
    "        # embed full fields as tokens\n",
    "        fields = einops.repeat(fields, \"b ... -> (b e) ...\", e = E, b = B)\n",
    "        tokens = self.to_tokens(fields)\n",
    "\n",
    "        # iterate\n",
    "        for s in range(S):\n",
    "            # prepare context and queries\n",
    "            context = tokens.gather(1, src_idx[s]) + self.src_positions(src_coo[s])\n",
    "            queries = self.tgt_positions(tgt_coo[s])\n",
    "\n",
    "            # maybe condition on noise\n",
    "            if exists(self.noise_generator):\n",
    "                context, noise = self.noise_generator(context, rng = rng)\n",
    "                queries = queries + noise.gather(1, tgt_idx[s])\n",
    "                context = context + noise.gather(1, src_idx[s])\n",
    "\n",
    "            # map context to latents\n",
    "            latents = einops.repeat(self.latents.weight, '... -> (b e) ...', b = B, e = E)\n",
    "            for read in self.encoder:\n",
    "                kv = torch.cat([context, latents], dim = 1)\n",
    "                latents = read(q = latents, kv = kv)\n",
    "\n",
    "            # map latents to queries\n",
    "            queries = self.decoder(q = queries, kv = latents)\n",
    "\n",
    "            # update tokens\n",
    "            tokens = tokens.scatter(1, tgt_idx[s], queries)\n",
    "\n",
    "        # map all tokens back to fields\n",
    "        fields = self.to_fields(tokens)\n",
    "        \n",
    "        # rearrange to ensemble form\n",
    "        fields = einops.rearrange(fields, \"(b e) ... k -> b ... (e k)\", e = E, b = B, k = K)\n",
    "        return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5936a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "ks = MultinomialMasking(world=world, objective=objective).to(device)\n",
    "model = EinMask(network, world).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "601684b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn((world.batch_size, *tuple(world.field_sizes[ax] for ax in world.field_layout)), device = device)\n",
    "src, mask, weight = ks((world.batch_size,))\n",
    "\n",
    "tgt = torch.stack([mask[i].nonzero().squeeze() for i in range(world.batch_size)])\n",
    "\n",
    "xs = model(data, src, tgt, members = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7628093f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a57e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
