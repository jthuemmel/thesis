{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94a2592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "from utils.components import *\n",
    "from einops.layers.torch import EinMix\n",
    "from utils.masking import *\n",
    "from utils.einmask import EinMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0732b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import *\n",
    "from omegaconf import OmegaConf\n",
    "from dataclasses import replace, dataclass\n",
    "from math import prod\n",
    "\n",
    "from utils.components import *\n",
    "from utils.config import *\n",
    "from utils.random_fields import RandomField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e54d1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = WorldConfig({'v': 6, 't':36,'h': 64, 'w':120}, {'vv':6, 'tt':6,'hh':4, 'ww':4}, batch_size=4)\n",
    "objective = ObjectiveConfig()\n",
    "network = NetworkConfig(512, 8, 256, dim_noise=32, num_tails=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "from utils.config import WorldConfig, ObjectiveConfig, default\n",
    "    \n",
    "class ForecastMasking(torch.nn.Module):\n",
    "    def __init__(self, world: WorldConfig, objective: ObjectiveConfig):\n",
    "        super().__init__()\n",
    "        self.world = world\n",
    "        self.objective = objective\n",
    "        self.event_pattern = 't'\n",
    "        self.register_buffer(\"prefix_frames\", torch.tensor(objective.tau, dtype = torch.long))\n",
    "        self.register_buffer(\"total_frames\", torch.tensor(world.token_sizes[\"t\"], dtype = torch.long))\n",
    "    \n",
    "    def forward(self, shape: tuple, return_indices: bool = True):\n",
    "        mask = torch.zeros((self.total_frames,), device = self.total_frames.device)\n",
    "        mask[:self.prefix_frames] = 1\n",
    "        mask = einops.repeat(mask, f'{self.event_pattern} -> {self.world.flat_token_pattern}', **self.world.token_sizes)\n",
    "        if return_indices:\n",
    "            indices = mask.nonzero(as_tuple=True)[0]\n",
    "            return indices.expand(*shape,-1)\n",
    "        else:\n",
    "            return mask.expand(*shape, -1).bool().logical_not()\n",
    "    \n",
    "# KUMARASWAMY DISTRIBUTION\n",
    "class StableKumaraswamy(torch.nn.Module):\n",
    "    '''Numerically stable methods for Kumaraswamy sampling, courtesy of Wasserman et al 2024'''\n",
    "    def __init__(self, c1: float = 1., c0: float = 1., epsilon=1e-3):\n",
    "        super().__init__()\n",
    "        assert c1 > 0. and c0 > 0., 'invalid concentration'\n",
    "       \n",
    "        # Register hyperparameters as buffers for device consistency\n",
    "        self.register_buffer(\"c1\", torch.as_tensor(c1))\n",
    "        self.register_buffer(\"c0\", torch.as_tensor(c0))\n",
    "        self.register_buffer(\"epsilon\", torch.as_tensor(epsilon))\n",
    "    \n",
    "    # Kumaraswamy with log1mexp\n",
    "    @staticmethod\n",
    "    def log1mexp(t: torch.FloatTensor): #numerically stable log(1 - e**x)\n",
    "        return torch.where(\n",
    "        t < -0.69314718, #~ -log2\n",
    "        torch.log1p(-torch.exp(t)), \n",
    "        torch.log(-torch.expm1(t))\n",
    "    )\n",
    "\n",
    "    def quantile_dt(self, t: torch.Tensor): # time derivative of the quantile function\n",
    "        #(1 - t)**((1 - c0) / c0) * (1 - (1 - t)**(1 / c0))**((1 - c1) / c1) / (c1 * c0)\n",
    "        log_1_minus_t = torch.log1p(-t) # 1 - t\n",
    "        log_constant = -self.c1.log() - self.c0.log() # 1 / c0 * c1\n",
    "        log_outer = log_1_minus_t * ((1 - self.c0) / self.c0) # (1 - t)**(1-c0)/c0\n",
    "        log_inner = ((1 - self.c1) / self.c1) * self.log1mexp(log_1_minus_t / self.c0)\n",
    "        return torch.exp(log_constant + log_inner + log_outer)\n",
    "\n",
    "    def quantile(self, t: torch.Tensor): # (1 - (1 - t)**(1 / c0))**(1 / c1)\n",
    "        return torch.exp(self.log1mexp(torch.log1p(-t) / self.c0) / self.c1)\n",
    "    \n",
    "    def cdf(self, t: torch.Tensor): # 1 - (1 - t**c1)**c0\n",
    "        return -torch.expm1(self.c0 * self.log1mexp(self.c1 * t.log()))\n",
    "    \n",
    "    def forward(self, shape: tuple, rng: torch.Generator = None):\n",
    "        t = torch.rand(shape, device=self.epsilon.device, generator= rng)\n",
    "        t = t * (1.0 - 2.0 * self.epsilon) + self.epsilon\n",
    "        return self.quantile(t), self.quantile_dt(t)\n",
    "\n",
    "# MASKING STRATEGIES\n",
    "class MultinomialMasking(torch.nn.Module):\n",
    "    def __init__(self, world: WorldConfig, objective: ObjectiveConfig):\n",
    "        super().__init__()\n",
    "        # configs\n",
    "        self.world = world\n",
    "        self.objective = objective\n",
    "\n",
    "        #schedule\n",
    "        self.schedule = StableKumaraswamy(c0=objective.c0, c1=objective.c1)\n",
    "        self.prior = StableKumaraswamy(c1= objective.alpha)\n",
    "\n",
    "        # attributes\n",
    "        self.k_min = default(objective.k_min, 1)\n",
    "        self.k_max = default(objective.k_max, world.num_tokens)\n",
    "\n",
    "        # Events\n",
    "        assert all([d in world.flat_token_pattern for d in objective.event_dims]), 'event dims not in token pattern'\n",
    "        self.num_events = torch.tensor([world.token_sizes[d] for d in objective.event_dims]).prod()\n",
    "        self.event_pattern = f'({\" \".join(objective.event_dims)})'\n",
    "\n",
    "    def forward(self, shape: tuple, rng: torch.Generator = None):\n",
    "        p, _ = self.prior((*shape, self.num_events), rng)\n",
    "        p = einops.repeat(p, f'... {self.event_pattern} -> ... {self.world.flat_token_pattern}', \n",
    "                             **self.world.token_sizes)\n",
    "        r, w = self.schedule((1,), rng)\n",
    "        k = self.k_min + (self.k_max - self.k_min) * r\n",
    "        indices = torch.multinomial(p, int(k), generator=rng)\n",
    "        mask = torch.ones_like(p, dtype= torch.bool).scatter_(1, indices, False)\n",
    "        return indices, mask, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5936a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "ks = MultinomialMasking(world=world, objective=objective).to(device)\n",
    "model = EinMask(network, world).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54156d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 36, 64, 120])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn((world.batch_size, *tuple(world.field_sizes[ax] for ax in world.field_layout)), device = device)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ccd107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src, _, weight = ks((world.batch_size,))\n",
    "tgt, mask, _ = ks((world.batch_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "601684b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = model(data, src, tgt, members = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7628093f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a57e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
