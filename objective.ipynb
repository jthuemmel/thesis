{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91298168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "import math\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from utils.model import *\n",
    "from utils.config import NetworkConfig\n",
    "from utils.loss_fn import f_kernel_crps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f93ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4339d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class WorldConfig:\n",
    "    field_sizes: dict\n",
    "    patch_sizes: dict\n",
    "    batch_size: int\n",
    "    tau: int\n",
    "    alphas: dict\n",
    "\n",
    "@dataclass\n",
    "class ConfigWrapper:\n",
    "    world : WorldConfig\n",
    "    network : NetworkConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f0a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = NetworkConfig(\n",
    "    dim = 256, \n",
    "    num_layers=4, \n",
    "    num_compute_blocks=4,\n",
    "    dim_in = 480,\n",
    "    dim_out = 960,\n",
    "    dim_coords = 128,\n",
    "    wavelengths=[(1e-3, 1e2,), (1e-3, 1e2), (1e-3, 1e2,), (1e-3, 1e2,)], \n",
    "    num_features= 9, \n",
    "    num_latents=64)\n",
    "\n",
    "world_cfg = WorldConfig(\n",
    "    field_sizes = {'v': 9, 't': 36, 'h': 64, 'w': 120},\n",
    "    patch_sizes = {'vv': 1, 'tt': 6, 'hh': 8, 'ww': 10},\n",
    "    batch_size = 8,\n",
    "    tau = 2,\n",
    "    alphas = {'t': 0.5},\n",
    ")\n",
    "\n",
    "cfg = ConfigWrapper(\n",
    "    world = world_cfg,\n",
    "    network = model_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a24638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteDiffusion(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.device_type = 'cpu'\n",
    "        self.device = torch.device('cpu')\n",
    "        self.network = MaskedTokenModel(cfg.network)\n",
    "\n",
    "        # Config shapes\n",
    "        self.batch_size = cfg.world.batch_size\n",
    "        self.field_sizes = cfg.world.field_sizes\n",
    "        self.patch_sizes = cfg.world.patch_sizes\n",
    "        self.field_layout = tuple(self.field_sizes.keys())\n",
    "        self.patch_layout = tuple(self.patch_sizes.keys())\n",
    "\n",
    "        # derived sizes and shapes\n",
    "        self.token_sizes = {ax: (self.field_sizes[ax] // self.patch_sizes[f'{ax*2}'])\n",
    "                            for ax in self.field_layout}\n",
    "        self.token_shape = tuple(self.token_sizes[ax] for ax in self.field_layout)\n",
    "        self.num_tokens = math.prod(self.token_sizes[ax] for ax in self.field_layout)\n",
    "        self.num_elements = math.prod(self.field_sizes[ax] for ax in self.field_layout)\n",
    "        self.dim_tokens = math.prod(self.patch_sizes[ax] for ax in self.patch_layout)\n",
    "\n",
    "        #einops patterns\n",
    "        field = \" \".join([f\"({f} {p})\" for f, p in zip(self.field_layout, self.patch_layout)])\n",
    "        self.field_pattern = f\"b {field}\"\n",
    "        self.flat_token_pattern = f\"({' '.join(self.field_layout)})\"\n",
    "        self.flat_patch_pattern = f\"({' '.join(self.patch_layout)})\"\n",
    "        self.flatland_pattern = f\"b {self.flat_token_pattern} {self.flat_patch_pattern}\"\n",
    "\n",
    "        # Index tensors: register as buffers so .to() moves them\n",
    "        flatland_index = torch.arange(self.num_tokens, device=self.device).expand(self.batch_size, -1)\n",
    "        self.register_buffer(\"flatland_index\", flatland_index)\n",
    "        token_index = torch.stack(torch.unravel_index(flatland_index, self.token_shape), dim=-1)\n",
    "        self.register_buffer(\"token_index\", token_index)\n",
    "\n",
    "        # additional config attributes\n",
    "        self.alphas = cfg.world.alphas\n",
    "        self.tau = cfg.world.tau\n",
    "\n",
    "    ### TOKENIZATION\n",
    "    def field_to_tokens(self, field):\n",
    "        return einops.rearrange(field, f'{self.field_pattern} -> {self.flatland_pattern}', **self.patch_sizes)\n",
    "    \n",
    "    def tokens_to_field(self, patch):\n",
    "        return einops.rearrange(patch, f\"{self.flatland_pattern} ... -> {self.field_pattern} ...\", **self.token_sizes, **self.patch_sizes)\n",
    "\n",
    "    ### MASKING\n",
    "    def gumbel_noise(self, shape: tuple, eps: float = 1e-6):\n",
    "        u = torch.rand(shape, device = self.device).clamp(min=eps, max=1-eps)\n",
    "        return -torch.log(-torch.log(u))\n",
    "            \n",
    "    def dirichlet_marginal(self, ax: str, eps: float = 1e-6):\n",
    "        concentration = torch.full((self.batch_size, self.token_sizes[ax]), self.alphas[ax], device= self.device)\n",
    "        probs = torch._sample_dirichlet(concentration).clamp(min=eps, max=1-eps)\n",
    "        probs = einops.repeat(probs, f'b {ax} -> b {self.flat_token_pattern}', **self.token_sizes)\n",
    "        return probs.log()\n",
    "    \n",
    "    def k_from_rates(self, rates):\n",
    "        return (self.num_tokens * rates).long().clamp(1, self.num_tokens - 1).view(-1, 1)\n",
    "            \n",
    "    def binary_topk(self, weights, ks):\n",
    "        index = weights.argsort(dim = -1, descending=True)\n",
    "        topk = self.flatland_index < ks\n",
    "        binary = torch.zeros_like(topk, dtype=torch.bool).scatter(1, index, topk)\n",
    "        return binary\n",
    "    \n",
    "    ### PRIORS\n",
    "    def get_visible_ws(self):\n",
    "        G = self.gumbel_noise((self.batch_size, self.num_tokens))\n",
    "        D = self.dirichlet_marginal('t')\n",
    "        return G + D\n",
    "\n",
    "    def get_history_ws(self):\n",
    "        step = torch.zeros((self.token_sizes['t'],), device=self.device)\n",
    "        step[:self._cfg.tau] = float('inf')\n",
    "        return einops.repeat(step, f't -> b {self.flat_token_pattern}', **self.token_sizes, b=self.batch_size)\n",
    "    \n",
    "    def get_visible_ks(self):\n",
    "        linear_grid = torch.linspace(0, 1, self.batch_size, device= self.device)\n",
    "        u = torch.rand((1,), device = self.device)\n",
    "        rates = (u + linear_grid) % 1 \n",
    "        return self.k_from_rates(rates)\n",
    "    \n",
    "    def get_history_ks(self):\n",
    "        rates = torch.full((self.batch_size,), self.tau / self.token_sizes['t'], device = self.device)\n",
    "        return self.k_from_rates(rates)\n",
    "\n",
    "    ### FORWARD\n",
    "    @property\n",
    "    def land_sea_mask(self):\n",
    "        lsm = torch.ones((1, self.field_sizes['h'], self.field_sizes['w']), device = self.device, dtype= torch.bool)\n",
    "        return einops.repeat(lsm, f'1 (h hh) (w ww) -> {self.flatland_pattern}', **self.patch_sizes, **self.token_sizes, b=self.batch_size)\n",
    "    \n",
    "    def forward(self, data, mode: str = 'prior'):\n",
    "        # tokens\n",
    "        data = data.to(self.device)\n",
    "        tokens = self.field_to_tokens(data)\n",
    "        \n",
    "        # masks\n",
    "        ws = self.get_visible_ws() if mode == 'prior' else self.get_history_ws()  \n",
    "        ks = self.get_visible_ks() if mode == 'prior' else self.get_history_ks() \n",
    "        visible = self.binary_topk(ws, ks)[..., None] # add singleton D dimension\n",
    "        \n",
    "        # predict\n",
    "        pred = self.network(tokens, visible, self.token_index)\n",
    "        \n",
    "        # scoring rule\n",
    "        ensemble = einops.rearrange(pred, '(b n) ... (d e) -> b ... d (n e)', d = tokens.size(-1), b = self.batch_size)\n",
    "        score = f_kernel_crps(tokens, ensemble)\n",
    "\n",
    "        # masked loss\n",
    "        mask = torch.logical_and(self.land_sea_mask, ~visible) # combine lsm [B, N, D] and mask [B, N, 1]\n",
    "        mask_rate = einops.reduce(mask, 'b n d -> b 1 1', reduction='sum') / self.num_elements\n",
    "        loss = (score * mask / mask_rate).sum() / mask.sum()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f3458a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = DiscreteDiffusion(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2a3911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.randn((dd.batch_size, *dd.field_sizes.values()))\n",
    "loss = dd(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74b55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cbfcef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
