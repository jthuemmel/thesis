{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4330e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.nn import Module, Embedding, Linear, ModuleList\n",
    "from einops import rearrange, reduce, repeat\n",
    "from utils.components import TransformerBlock, ConditionalLayerNorm, GatedFFN, Attention\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "from utils.multimodal import ModalMTM, ModalTailMTM, ModalFuncMTM\n",
    "from utils.loss_fn import f_kernel_crps\n",
    "\n",
    "from torch import autocast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d90d7a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_alpha(weights: list, sharpness: float = 10.0) -> torch.Tensor:\n",
    "    weights = torch.tensor(weights)\n",
    "    probs = weights / weights.sum()  # normalize to probability simplex\n",
    "    alpha = probs * sharpness\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dirichlet(shape:Tuple[int, ...],\n",
    "                     alpha: float | torch.Tensor, \n",
    "                     device: Optional[torch.device] = None, \n",
    "                     generator: Optional[torch.Generator] = None\n",
    "                     ) -> torch.Tensor:\n",
    "    concentration = alpha * torch.ones(shape, device=device) \n",
    "    return torch._sample_dirichlet(concentration, generator=generator)\n",
    "\n",
    "def sample_truncated_normal(shape: Tuple[int, ...] = (1,),\n",
    "                             mean: float = 0.0, \n",
    "                             std: float = 1.0, \n",
    "                             a: float = -2.0, \n",
    "                             b: float = 2.0,\n",
    "                            device: Optional[torch.device] = None, \n",
    "                            generator: Optional[torch.Generator] = None\n",
    "                            ) -> torch.Tensor:\n",
    "    return torch.nn.init.trunc_normal_(torch.empty(shape, device=device), mean=mean, std=std, a=a, b=b, generator=generator)\n",
    "\n",
    "def sample_multinomial(weight: torch.Tensor, \n",
    "                       num_samples: int = 1, \n",
    "                       replacement: bool = False,\n",
    "                        generator: Optional[torch.Generator] = None\n",
    "                       ) -> torch.Tensor:\n",
    "    return torch.multinomial(weight, num_samples=num_samples, replacement=replacement, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2276af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b6797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import repeat\n",
    "\n",
    "B, V, T, S = 4096, 4, 6, 256\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "W_time = sample_dirichlet((B, T), alpha=0.5, device=device)\n",
    "W_spatiotemporal = repeat(W_time, 'b t -> b (t s)', s=S)\n",
    "\n",
    "p_total = sample_truncated_normal((1,), mean=0.1, std=0.1, a=0.01, b=0.5, device=device)\n",
    "R_total = (p_total * V * T * S).long()\n",
    "\n",
    "alpha = construct_alpha([2., 0.5, 0.25, 0.02], sharpness=150)\n",
    "r_vars = sample_dirichlet((V,), alpha=alpha, device=device)  # (B, V)\n",
    "\n",
    "num_samples_per_var = (r_vars * R_total).long() # (B, V)\n",
    "\n",
    "masks = torch.cat([sample_multinomial(W_spatiotemporal, num_samples=r) for r in num_samples_per_var], dim=-1)  # (B, sum(num_samples_per_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "534c03b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = sample_dirichlet((10000, 5), construct_alpha([1.,], sharpness=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "751a6efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = plt.hist(W, bins = 10**np.linspace(-2, 0, 100), density=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f27b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ambient_src(self, src, tgt, latents, ctx):\n",
    "    # O(2*z*(src+tgt) + z**2)\n",
    "    x = torch.cat([src, tgt], dim = 1)\n",
    "    for block in self.network:\n",
    "        x, z = block(x = x, z = latents, ctx = ctx)\n",
    "    _, tgt = x.split([src.size(1), tgt.size(1)], dim = 1)\n",
    "    return tgt, z\n",
    "\n",
    "def predict_latent_src(self, src, tgt, latents, ctx):\n",
    "    #O(2*tgt*(src + z) + (src * z)**2)\n",
    "    z = torch.cat([src, latents], dim = 1)\n",
    "    for block in self.network:\n",
    "        tgt, z = block(x = tgt, z = z, ctx = ctx)\n",
    "    _, z = z.split([src.size(1), latents.size(1)], dim=1)\n",
    "    return tgt, z\n",
    "\n",
    "def predict_self_vit(self, src, tgt, latents, ctx):\n",
    "    x = torch.cat([src, latents], dim = 1)\n",
    "    encoder, decoder = self.network[:-1], self.network[-1]\n",
    "    for block in encoder:\n",
    "        x, _ = block(x, x, ctx)\n",
    "    q = torch.cat([x, tgt], dim = 1)\n",
    "    q = decoder(q, q, ctx)\n",
    "    _, tgt = q.split([x.size(1), tgt.size(1)], dim = 1)\n",
    "    return tgt, None\n",
    "\n",
    "def predict_cross_vit(self, src, tgt, latents, ctx):\n",
    "    x = torch.cat([src, latents], dim = 1)\n",
    "    encoder, decoder = self.network[:-1], self.network[-1]\n",
    "    for block in encoder:\n",
    "        x, _ = block(x, x, ctx)\n",
    "    tgt = decoder(tgt, x, ctx)\n",
    "    return tgt, None\n",
    "\n",
    "def predict_perceiver(self, src, tgt, latents, ctx):\n",
    "    encoder, decoder = self.network[:-1], self.network[-1]\n",
    "    for block in encoder:\n",
    "        _, z = block(src, latents, ctx)\n",
    "    tgt = decoder(tgt, z, ctx)\n",
    "    return tgt, None\n",
    "\n",
    "def forward(self, src, src_pos, tgt_pos, ctx = None):  \n",
    "        #initialize context\n",
    "        ctx = self.proj_noise(ctx if ctx is not None else src.new_zeros(self.dim_ctx))\n",
    "        # initialize src\n",
    "        src = self.proj_in(src) + self.positions(src_pos)\n",
    "        # initialize latents\n",
    "        latents = repeat(self.latents.weight, \"z d -> b z d\", b = src.size(0))      \n",
    "        # initialize tgt\n",
    "        tgt = self.queries(torch.zeros_like(tgt_pos)) + self.positions(tgt_pos)\n",
    "        # predict\n",
    "        tgt, _ = self.predict_latent_src(src, tgt, latents, ctx)\n",
    "        # project out\n",
    "        tgt = self.proj_out(tgt)\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91240848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2253f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    dim_in: int\n",
    "    dim_out: int\n",
    "    dim: int\n",
    "    num_tokens: Optional[int] = None\n",
    "    num_compute_blocks: Optional[int] = None\n",
    "    num_layers: Optional[int] = None\n",
    "    num_latents: Optional[int] = None\n",
    "    dim_heads: int = 64\n",
    "    dim_noise: int = 8\n",
    "    dim_coords: int = 32\n",
    "    architecture: str = \"interface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d186098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.field_network import NeuralWeatherField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b9914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, N, F, C = 16, 4096, 73, 4\n",
    "D = 512\n",
    "K = 4\n",
    "M_src, M_tgt, F_src, F_tgt = 512, 1024, 32, 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74492a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(C, C, D, N, 4, 4, 256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98b893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66,839,628\n"
     ]
    }
   ],
   "source": [
    "model = NeuralWeatherField(cfg).to(device)\n",
    "\n",
    "compiled = torch.compile(\n",
    "    model, \n",
    "    fullgraph=False,\n",
    "    dynamic=True,\n",
    "    disable=False\n",
    "    )\n",
    "\n",
    "optim = torch.optim.AdamW(compiled.parameters())\n",
    "print(f'{sum(p.numel() for p in compiled.parameters()):,}') \n",
    "\n",
    "def step():\n",
    "    with autocast(device, torch.float16):\n",
    "        M_src = torch.randint(64, 512, (1,), device=device).item()\n",
    "        M_tgt = torch.randint(768, 1280,(1,), device= device).item()\n",
    "\n",
    "        noise = torch.randn((K * B, 1, 8), device = device)\n",
    "\n",
    "        pos_src = torch.multinomial(torch.ones((B, N), device = device), M_src)\n",
    "        pos_tgt = torch.multinomial(torch.ones((B, N), device = device), M_tgt)\n",
    "        y = torch.randn((B, M_tgt, C), device = device)\n",
    "        x = torch.randn((B, M_src, C), device = device)\n",
    "        \n",
    "        x, pos_src, pos_tgt = (repeat(var, \"b ... -> (b k) ...\", k = K) for var in (x, pos_src, pos_tgt))\n",
    "        x_hat, z_hat = compiled(x, pos_src, pos_tgt, noise=noise)\n",
    "        x_hat = rearrange(x_hat, '(b k) ... -> b ... k', k = K)\n",
    "        loss = f_kernel_crps(y, x_hat).mean()\n",
    "        \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c38fdb5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensor type unknown to einops <class 'tuple'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n",
      "Cell \u001b[0;32mIn[19], line 27\u001b[0m, in \u001b[0;36mstep\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     x, pos_src, pos_tgt \u001b[38;5;241m=\u001b[39m (repeat(var, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb ... -> (b k) ...\u001b[39m\u001b[38;5;124m\"\u001b[39m, k \u001b[38;5;241m=\u001b[39m K) \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m (x, pos_src, pos_tgt))\n\u001b[1;32m     26\u001b[0m     x_hat \u001b[38;5;241m=\u001b[39m compiled(x, pos_src, pos_tgt, noise\u001b[38;5;241m=\u001b[39mnoise)\n\u001b[0;32m---> 27\u001b[0m     x_hat \u001b[38;5;241m=\u001b[39m \u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(b k) ... -> b ... k\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m f_kernel_crps(y, x_hat)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     30\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/mnt/lustre/work/ludwig/jthuemmel54/miniforge3/envs/torch/lib/python3.13/site-packages/einops/einops.py:600\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrearrange\u001b[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maxes_lengths: Size) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m \n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrearrange\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/lustre/work/ludwig/jthuemmel54/miniforge3/envs/torch/lib/python3.13/site-packages/einops/einops.py:527\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    525\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mstack_on_zeroth_dimension(tensor)\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m hashable_axes_lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(axes_lengths\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    530\u001b[0m shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mshape(tensor)\n",
      "File \u001b[0;32m/mnt/lustre/work/ludwig/jthuemmel54/miniforge3/envs/torch/lib/python3.13/site-packages/einops/_backends.py:59\u001b[0m, in \u001b[0;36mget_backend\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 _type2backend[_type] \u001b[38;5;241m=\u001b[39m backend\n\u001b[1;32m     57\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor type unknown to einops \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(tensor)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor type unknown to einops <class 'tuple'>"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    step()\n",
    "elapsed = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037647a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bda57eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected a cuda device, but got: cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_peak_memory_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      4\u001b[0m     step()\n",
      "File \u001b[0;32m/mnt/lustre/work/ludwig/jthuemmel54/miniforge3/envs/torch/lib/python3.13/site-packages/torch/cuda/memory.py:369\u001b[0m, in \u001b[0;36mreset_peak_memory_stats\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset_peak_memory_stats\u001b[39m(device: Union[Device, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Reset the \"peak\" stats tracked by the CUDA memory allocator.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m    See :func:`~torch.cuda.memory_stats` for details. Peak stats correspond to the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m        management.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43m_get_device_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_resetPeakMemoryStats(device)\n",
      "File \u001b[0;32m/mnt/lustre/work/ludwig/jthuemmel54/miniforge3/envs/torch/lib/python3.13/site-packages/torch/cuda/_utils.py:34\u001b[0m, in \u001b[0;36m_get_device_index\u001b[0;34m(device, optional, allow_cpu)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a cuda or cpu device, but got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a cuda device, but got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice):\n",
      "\u001b[0;31mValueError\u001b[0m: Expected a cuda device, but got: cpu"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "torch.cuda.reset_peak_memory_stats(device)\n",
    "for _ in range(100):\n",
    "    step()\n",
    "elapsed = time.time() - start\n",
    "peak_alloc = torch.cuda.max_memory_allocated(device) / (1024**3)\n",
    "\n",
    "print(f\"  • 100‑run time     : {elapsed:.4f} s\")\n",
    "print(f\"  • Peak VRAM (100 runs): {peak_alloc:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb9f763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modal_cfg = Config(C, C, D, F, N)\n",
    "interface_cfg = Config(D, D, D, None, N, 4, 6, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30a8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d45a515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9dbf734",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'dim_noise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModalFuncMTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmtm_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterface_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodal_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodal_cfg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m compiled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      4\u001b[0m     model, \n\u001b[1;32m      5\u001b[0m     fullgraph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     dynamic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m     10\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(compiled\u001b[38;5;241m.\u001b[39mparameters())\n",
      "File \u001b[0;32m/mnt/lustre/work/ludwig/jthuemmel54/thesis/utils/multimodal.py:88\u001b[0m, in \u001b[0;36mModalFuncMTM.__init__\u001b[0;34m(self, modal_cfg, mtm_cfg)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, modal_cfg, mtm_cfg):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodal_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtm_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_ctx \u001b[38;5;241m=\u001b[39m Linear(mtm_cfg\u001b[38;5;241m.\u001b[39mdim_noise, mtm_cfg\u001b[38;5;241m.\u001b[39mdim_noise)\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mtrunc_normal_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_ctx\u001b[38;5;241m.\u001b[39mweight, std \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.02\u001b[39m)\n",
      "File \u001b[0;32m/mnt/lustre/work/ludwig/jthuemmel54/thesis/utils/multimodal.py:12\u001b[0m, in \u001b[0;36mModalMTM.__init__\u001b[0;34m(self, modal_cfg, mtm_cfg)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, modal_cfg: dataclass, mtm_cfg: dataclass):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mModalEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodal_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m ModalDecoder(modal_cfg)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;241m=\u001b[39m MTM(mtm_cfg)\n",
      "File \u001b[0;32m/mnt/lustre/work/ludwig/jthuemmel54/thesis/utils/networks.py:82\u001b[0m, in \u001b[0;36mModalEncoder.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_bias \u001b[38;5;241m=\u001b[39m Embedding(cfg\u001b[38;5;241m.\u001b[39mnum_features, cfg\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueries \u001b[38;5;241m=\u001b[39m Embedding(\u001b[38;5;241m1\u001b[39m, cfg\u001b[38;5;241m.\u001b[39mdim)       \n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_norm \u001b[38;5;241m=\u001b[39m ConditionalLayerNorm(cfg\u001b[38;5;241m.\u001b[39mdim, \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_noise\u001b[49m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn \u001b[38;5;241m=\u001b[39m TransformerBlock(cfg\u001b[38;5;241m.\u001b[39mdim, dim_ctx\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdim_noise)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Config' object has no attribute 'dim_noise'"
     ]
    }
   ],
   "source": [
    "model = ModalFuncMTM(mtm_cfg=interface_cfg, modal_cfg=modal_cfg).to(device)\n",
    "\n",
    "compiled = torch.compile(\n",
    "    model, \n",
    "    fullgraph=False,\n",
    "    dynamic=True,\n",
    "    disable=False\n",
    "    )\n",
    "\n",
    "optim = torch.optim.AdamW(compiled.parameters())\n",
    "print(f'{sum(p.numel() for p in compiled.parameters()):,}') \n",
    "\n",
    "def step():\n",
    "    with autocast(device, torch.float16):\n",
    "        M_src = torch.randint(64, 512, (1,), device=device).item()\n",
    "        M_tgt = torch.randint(768, 1280,(1,), device= device).item()\n",
    "        F_src = torch.randint(16, 32,(1,), device=device).item()\n",
    "        F_tgt = torch.randint(32, 64,(1,), device= device).item()\n",
    "\n",
    "        pos_src = torch.multinomial(torch.ones((B, N), device = device), M_src)\n",
    "        pos_tgt = torch.multinomial(torch.ones((B, N), device = device), M_tgt)\n",
    "        var_src = torch.multinomial(torch.ones((B, F), device = device), F_src)\n",
    "        var_tgt = torch.multinomial(torch.ones((B, F), device = device), F_tgt)\n",
    "        \n",
    "        noise = torch.randn((K * B, 1, 8), device = device)\n",
    "\n",
    "        y = torch.randn((B, M_tgt, F_tgt, C), device = device)\n",
    "        x = torch.randn((B, M_src, F_src, C), device = device)\n",
    "        \n",
    "        x_hat = compiled(x, pos_src = pos_src, pos_tgt = pos_tgt, var_src= var_src, var_tgt=var_tgt, noise=noise)\n",
    "        loss = f_kernel_crps(y, x_hat).mean()\n",
    "        \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce3844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • 100‑run time     : 37.7447 s\n",
      "  • Peak VRAM (100 runs): 28.2 GB\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "torch.cuda.reset_peak_memory_stats(device)\n",
    "for _ in range(100):\n",
    "    step()\n",
    "elapsed = time.time() - start\n",
    "peak_alloc = torch.cuda.max_memory_allocated(device) / (1024**3)\n",
    "\n",
    "print(f\"  • 100‑run time     : {elapsed:.4f} s\")\n",
    "print(f\"  • Peak VRAM (100 runs): {peak_alloc:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264744a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ModalTailMTM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModalTailMTM\u001b[49m(mtm_cfg\u001b[38;5;241m=\u001b[39minterface_cfg, modal_cfg\u001b[38;5;241m=\u001b[39mmodal_cfg)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m compiled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      4\u001b[0m     model, \n\u001b[1;32m      5\u001b[0m     fullgraph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     dynamic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m     10\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(compiled\u001b[38;5;241m.\u001b[39mparameters())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ModalTailMTM' is not defined"
     ]
    }
   ],
   "source": [
    "model = ModalTailMTM(mtm_cfg=interface_cfg, modal_cfg=modal_cfg).to(device)\n",
    "\n",
    "compiled = torch.compile(\n",
    "    model, \n",
    "    fullgraph=False,\n",
    "    dynamic=True,\n",
    "    disable=False\n",
    "    )\n",
    "\n",
    "optim = torch.optim.AdamW(compiled.parameters())\n",
    "print(f'{sum(p.numel() for p in compiled.parameters()):,}') \n",
    "\n",
    "def step():\n",
    "    with autocast(device, torch.float16):\n",
    "        M_src = torch.randint(64, 512, (1,), device=device).item()\n",
    "        M_tgt = torch.randint(768, 1280,(1,), device= device).item()\n",
    "        F_src = torch.randint(16, 32,(1,), device=device).item()\n",
    "        F_tgt = torch.randint(32, 64,(1,), device= device).item()\n",
    "\n",
    "        pos_src = torch.multinomial(torch.ones((B, N), device = device), M_src)\n",
    "        pos_tgt = torch.multinomial(torch.ones((B, N), device = device), M_tgt)\n",
    "        var_src = torch.multinomial(torch.ones((B, F), device = device), F_src)\n",
    "        var_tgt = torch.multinomial(torch.ones((B, F), device = device), F_tgt)\n",
    "        \n",
    "        noise = torch.randn((K * B, 1, 8), device = device)\n",
    "\n",
    "        y = torch.randn((B, M_tgt, F_tgt, C), device = device)\n",
    "        x = torch.randn((B, M_src, F_src, C), device = device)\n",
    "        \n",
    "        x_hat = compiled(x, pos_src = pos_src, pos_tgt = pos_tgt, var_src= var_src, var_tgt=var_tgt, noise=noise)\n",
    "        loss = f_kernel_crps(y, x_hat).mean()\n",
    "        \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d474cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • 100‑run time     : 14.4890 s\n",
      "  • Peak VRAM (100 runs): 8.7 GB\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "torch.cuda.reset_peak_memory_stats(device)\n",
    "for _ in range(100):\n",
    "    step()\n",
    "elapsed = time.time() - start\n",
    "peak_alloc = torch.cuda.max_memory_allocated(device) / (1024**3)\n",
    "\n",
    "print(f\"  • 100‑run time     : {elapsed:.4f} s\")\n",
    "print(f\"  • Peak VRAM (100 runs): {peak_alloc:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "599c2cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102,783,488\n"
     ]
    }
   ],
   "source": [
    "model = ModalMTM(mtm_cfg=interface_cfg, modal_cfg=modal_cfg.__replace__(dim_out=C*K)).to(device)\n",
    "\n",
    "compiled = torch.compile(\n",
    "    model, \n",
    "    fullgraph=False,\n",
    "    dynamic=True,\n",
    "    disable=True\n",
    "    )\n",
    "\n",
    "optim = torch.optim.AdamW(compiled.parameters())\n",
    "print(f'{sum(p.numel() for p in compiled.parameters()):,}') \n",
    "\n",
    "def step():\n",
    "    with autocast(device, torch.float16):\n",
    "        M_src = torch.randint(64, 512, (1,), device=device).item()\n",
    "        M_tgt = torch.randint(768, 1280,(1,), device= device).item()\n",
    "        F_src = torch.randint(16, 32,(1,), device=device).item()\n",
    "        F_tgt = torch.randint(32, 64,(1,), device= device).item()\n",
    "\n",
    "        pos_src = torch.multinomial(torch.ones((B, N), device = device), M_src)\n",
    "        pos_tgt = torch.multinomial(torch.ones((B, N), device = device), M_tgt)\n",
    "        var_src = torch.multinomial(torch.ones((B, F), device = device), F_src)\n",
    "        var_tgt = torch.multinomial(torch.ones((B, F), device = device), F_tgt)\n",
    "        \n",
    "        y = torch.randn((B, M_tgt, F_tgt, C), device = device)\n",
    "        x = torch.randn((B, M_src, F_src, C), device = device)\n",
    "        \n",
    "        x_hat = compiled(x, pos_src = pos_src, pos_tgt = pos_tgt, var_src= var_src, var_tgt=var_tgt)\n",
    "        x_hat = rearrange(x_hat, '... (c k) -> ... c k', k = K)\n",
    "        loss = f_kernel_crps(y, x_hat).mean()\n",
    "        \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ea52b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • 100‑run time     : 13.9080 s\n",
      "  • Peak VRAM (100 runs): 8.2 GB\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "torch.cuda.reset_peak_memory_stats(device)\n",
    "for _ in range(100):\n",
    "    step()\n",
    "elapsed = time.time() - start\n",
    "peak_alloc = torch.cuda.max_memory_allocated(device) / (1024**3)\n",
    "\n",
    "print(f\"  • 100‑run time     : {elapsed:.4f} s\")\n",
    "print(f\"  • Peak VRAM (100 runs): {peak_alloc:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc735c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f83873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404e469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d5380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • torch.profiler summary:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Torch-Compiled Region: 0/0         0.24%      34.862ms        15.42%        2.262s      46.156ms       0.000us         0.00%        2.477s      50.547ms      28.33 Kb           0 b     768.30 Gb           0 b            49  \n",
      "                                       CompiledFunction         4.66%     683.877ms        15.18%        2.227s      45.444ms       0.000us         0.00%        2.477s      50.547ms      28.33 Kb           0 b     768.30 Gb     737.88 Gb            49  \n",
      "                                            aten::empty         0.61%      89.912ms         0.61%      89.912ms       4.946us       0.000us         0.00%       0.000us       0.000us      28.51 Kb      28.51 Kb     330.95 Gb     330.95 Gb         18179  \n",
      "                                    aten::empty_strided         1.21%     177.607ms         1.21%     177.607ms       5.230us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     199.49 Gb     199.49 Gb         33957  \n",
      "                                       aten::empty_like         0.08%      11.404ms         0.37%      53.903ms       7.190us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     165.48 Gb           0 b          7497  \n",
      "     aten::_scaled_dot_product_flash_attention_backward         0.17%      25.228ms         3.45%     506.401ms     279.317us       0.000us         0.00%        1.612s     889.338us           0 b           0 b     135.90 Gb           0 b          1813  \n",
      "                        aten::_flash_attention_backward         0.23%      34.405ms         3.11%     455.975ms     251.503us        1.612s        17.96%        1.612s     889.338us           0 b           0 b     135.90 Gb    -269.07 Gb          1813  \n",
      "                                            aten::zeros         0.01%       1.426ms         0.07%      10.553ms      23.930us       0.000us         0.00%      36.596ms      82.985us           0 b           0 b      52.06 Gb           0 b           441  \n",
      "                                              aten::abs         0.03%       3.975ms         0.11%      15.438ms      31.506us      32.715ms         0.36%      65.430ms     133.531us           0 b           0 b      42.88 Gb           0 b           490  \n",
      "              aten::_scaled_dot_product_flash_attention         0.12%      18.080ms         0.87%     127.280ms      70.204us       0.000us         0.00%     385.415ms     212.584us      28.33 Kb           0 b      30.42 Gb           0 b          1813  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 14.666s\n",
      "Self CUDA time total: 8.980s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        profile_memory=True,\n",
    "        record_shapes=False,      # ← turn OFF shape logging\n",
    "        with_stack=False,         # ← turn OFF call‐stack\n",
    "        schedule=torch.profiler.schedule(wait=0, warmup=1, active=50, repeat=0)\n",
    "    ) as prof:\n",
    "        for _ in range(50):\n",
    "            step()\n",
    "            prof.step()\n",
    "\n",
    "\n",
    "print(\"  • torch.profiler summary:\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca3972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/work/ludwig/jthuemmel54/miniforge3/envs/torch/lib/python3.13/site-packages/torch/profiler/profiler.py:488: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • 100‑run time     : 30.0844 s\n",
      "  • Peak VRAM (100 runs): 23996.4 MiB\n",
      "  • torch.profiler summary:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::mul         0.89%       4.803ms         3.23%      17.426ms      38.812us      44.582ms        15.11%      44.582ms      99.291us           0 b           0 b      20.94 Gb      20.94 Gb           449  \n",
      "                                           aten::linear         0.54%       2.889ms        15.51%      83.642ms     154.322us       0.000us         0.00%      59.451ms     109.688us           0 b           0 b      16.68 Gb      -1.00 Gb           542  \n",
      "                                              aten::div         0.70%       3.749ms         2.49%      13.449ms      39.326us      30.272ms        10.26%      30.272ms      88.516us           0 b           0 b      16.56 Gb      16.56 Gb           342  \n",
      "                                    aten::empty_strided         2.16%      11.662ms         2.16%      11.662ms       5.678us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      15.56 Gb      15.56 Gb          2054  \n",
      "                                               aten::to         0.32%       1.699ms        11.57%      62.429ms      43.353us       0.000us         0.00%      26.358ms      18.304us           0 b           0 b      12.50 Gb           0 b          1440  \n",
      "                                         aten::_to_copy         1.17%       6.336ms        11.26%      60.729ms      42.202us       0.000us         0.00%      26.358ms      18.317us           0 b           0 b      12.50 Gb           0 b          1439  \n",
      "                                               aten::mm         2.91%      15.680ms         5.64%      30.447ms      50.159us      49.725ms        16.85%      49.725ms      81.919us           0 b           0 b       9.46 Gb       9.46 Gb           607  \n",
      "                                           MulBackward0         0.09%     476.704us         1.16%       6.237ms      60.555us       0.000us         0.00%      20.511ms     199.141us           0 b           0 b       9.31 Gb           0 b           103  \n",
      "                                            aten::empty         0.55%       2.986ms         0.55%       2.986ms       4.840us       0.000us         0.00%       0.000us       0.000us         532 b         532 b       8.32 Gb       8.32 Gb           617  \n",
      "                                        ToCopyBackward0         0.17%     930.714us         4.19%      22.626ms      37.031us       0.000us         0.00%      11.235ms      18.387us           0 b           0 b       6.57 Gb           0 b           611  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 539.377ms\n",
      "Self CUDA time total: 295.085ms\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "big ambient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7745ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6adfd81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279d9bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477bcb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/work/ludwig/jthuemmel54/miniforge3/envs/torch/lib/python3.13/site-packages/torch/profiler/profiler.py:488: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • 100‑run time     : 21.2701 s\n",
      "  • Peak VRAM (100 runs): 17726.2 MiB\n",
      "  • torch.profiler summary:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::mul         0.42%       2.006ms         3.14%      14.888ms      78.775us      32.266ms        15.10%      32.266ms     170.722us           0 b           0 b      15.05 Gb      15.05 Gb           189  \n",
      "                                              aten::div         0.34%       1.602ms         2.06%       9.733ms      68.541us      22.538ms        10.55%      22.538ms     158.717us           0 b           0 b      12.03 Gb      12.03 Gb           142  \n",
      "                                           aten::linear         0.24%       1.133ms         4.64%      21.993ms      99.068us       0.000us         0.00%      42.143ms     189.834us           0 b           0 b      11.60 Gb      -1.00 Gb           222  \n",
      "                                    aten::empty_strided         1.02%       4.849ms         1.02%       4.849ms       5.678us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      10.99 Gb      10.99 Gb           854  \n",
      "                                               aten::to         0.15%     725.253us         9.27%      43.894ms      73.157us       0.000us         0.00%      18.495ms      30.824us           0 b           0 b       9.03 Gb           0 b           600  \n",
      "                                         aten::_to_copy         0.54%       2.538ms         9.12%      43.169ms      72.068us       0.000us         0.00%      18.495ms      30.876us           0 b           0 b       9.03 Gb           0 b           599  \n",
      "                                            aten::empty         0.30%       1.438ms         0.30%       1.438ms       4.841us       0.000us         0.00%       0.000us       0.000us         212 b         212 b       7.60 Gb       7.60 Gb           297  \n",
      "                                               aten::mm         1.32%       6.259ms         7.32%      34.678ms     140.396us      33.291ms        15.58%      33.291ms     134.780us           0 b           0 b       6.65 Gb       6.65 Gb           247  \n",
      "                                           MulBackward0         0.08%     385.656us         2.18%      10.301ms     239.549us       0.000us         0.00%      14.568ms     338.792us           0 b           0 b       6.59 Gb           0 b            43  \n",
      "                                           DivBackward0         0.07%     327.155us         2.70%      12.766ms     455.937us       0.000us         0.00%      22.117ms     789.889us           0 b           0 b       4.69 Gb      -7.03 Gb            28  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 473.589ms\n",
      "Self CUDA time total: 213.666ms\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ambient src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d70a62b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/work/ludwig/jthuemmel54/miniforge3/envs/torch/lib/python3.13/site-packages/torch/profiler/profiler.py:488: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • 100‑run time     : 25.7852 s\n",
      "  • Peak VRAM (100 runs): 20819.7 MiB\n",
      "  • torch.profiler summary:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::mul         0.35%       2.006ms         3.09%      17.784ms      94.096us      40.035ms        15.49%      40.035ms     211.824us           0 b           0 b      18.30 Gb      18.30 Gb           189  \n",
      "                                              aten::div         0.27%       1.564ms         1.92%      11.086ms      78.073us      26.633ms        10.30%      26.633ms     187.555us           0 b           0 b      14.53 Gb      14.53 Gb           142  \n",
      "                                           aten::linear         0.19%       1.122ms         4.41%      25.407ms     114.447us       0.000us         0.00%      50.122ms     225.773us           0 b           0 b      14.35 Gb      -1.00 Gb           222  \n",
      "                                    aten::empty_strided         0.82%       4.750ms         0.82%       4.750ms       5.562us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      13.24 Gb      13.24 Gb           854  \n",
      "                                               aten::to         0.13%     734.802us         9.73%      56.061ms      93.436us       0.000us         0.00%      22.251ms      37.084us           0 b           0 b      10.78 Gb           0 b           600  \n",
      "                                         aten::_to_copy         0.44%       2.563ms         9.61%      55.327ms      92.365us       0.000us         0.00%      22.251ms      37.146us           0 b           0 b      10.78 Gb           0 b           599  \n",
      "                                               aten::mm         1.01%       5.795ms         6.29%      36.203ms     146.569us      40.897ms        15.82%      40.897ms     165.574us           0 b           0 b       8.15 Gb       8.15 Gb           247  \n",
      "                                           MulBackward0         0.03%     177.289us         2.04%      11.737ms     272.959us       0.000us         0.00%      18.074ms     420.330us           0 b           0 b       8.09 Gb           0 b            43  \n",
      "                                            aten::empty         0.24%       1.388ms         0.24%       1.388ms       4.722us       0.000us         0.00%       0.000us       0.000us         212 b         212 b       7.57 Gb       7.57 Gb           294  \n",
      "                                           DivBackward0         0.06%     320.638us         2.10%      12.094ms     431.920us       0.000us         0.00%      26.937ms     962.023us           0 b           0 b       5.69 Gb      -8.53 Gb            28  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 575.943ms\n",
      "Self CUDA time total: 258.499ms\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latent src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe83cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/work/ludwig/jthuemmel54/miniforge3/envs/torch/lib/python3.13/site-packages/torch/profiler/profiler.py:488: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • 100‑run time     : 24.6475 s\n",
      "  • Peak VRAM (100 runs): 19914.6 MiB\n",
      "  • torch.profiler summary:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::mul         0.45%       2.418ms         3.90%      21.074ms     101.808us      37.754ms        15.25%      37.754ms     182.386us           0 b           0 b      17.18 Gb      17.18 Gb           207  \n",
      "                                              aten::div         0.32%       1.757ms         3.57%      19.290ms     122.865us      24.706ms         9.98%      24.706ms     157.361us           0 b           0 b      13.71 Gb      13.71 Gb           157  \n",
      "                                           aten::linear         0.24%       1.288ms         4.26%      23.006ms      96.664us       0.000us         0.00%      46.950ms     197.269us           0 b           0 b      13.15 Gb      -1.00 Gb           238  \n",
      "                                    aten::empty_strided         0.98%       5.319ms         0.98%       5.319ms       5.653us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      11.79 Gb      11.79 Gb           941  \n",
      "                                               aten::to         0.14%     768.231us         6.83%      36.937ms      55.212us       0.000us         0.00%      19.878ms      29.713us           0 b           0 b       9.52 Gb           0 b           669  \n",
      "                                         aten::_to_copy         0.53%       2.848ms         6.69%      36.169ms      54.145us       0.000us         0.00%      19.878ms      29.757us           0 b           0 b       9.52 Gb           0 b           668  \n",
      "                                            aten::empty         0.31%       1.698ms         0.31%       1.698ms       4.965us       0.000us         0.00%       0.000us       0.000us         228 b         228 b       8.11 Gb       8.11 Gb           342  \n",
      "                                           MulBackward0         0.04%     205.076us         1.85%      10.021ms     213.221us       0.000us         0.00%      16.937ms     360.361us           0 b           0 b       7.57 Gb           0 b            47  \n",
      "                                               aten::mm         1.07%       5.800ms         4.24%      22.926ms      95.923us      37.931ms        15.32%      37.931ms     158.707us           0 b           0 b       7.52 Gb       7.52 Gb           239  \n",
      "                                           DivBackward0         0.07%     364.405us         4.45%      24.044ms     775.609us       0.000us         0.00%      25.310ms     816.449us           0 b           0 b       5.36 Gb      -8.04 Gb            31  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 540.550ms\n",
      "Self CUDA time total: 247.543ms\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "self vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3fb79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/work/ludwig/jthuemmel54/miniforge3/envs/torch/lib/python3.13/site-packages/torch/profiler/profiler.py:488: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • 100‑run time     : 24.0923 s\n",
      "  • Peak VRAM (100 runs): 19592.8 MiB\n",
      "  • torch.profiler summary:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::mul         0.41%       2.170ms         2.63%      13.929ms      67.290us      36.885ms        15.26%      36.885ms     178.190us           0 b           0 b      16.78 Gb      16.78 Gb           207  \n",
      "                                              aten::div         0.33%       1.747ms         2.93%      15.495ms      98.691us      24.149ms         9.99%      24.149ms     153.818us           0 b           0 b      13.40 Gb      13.40 Gb           157  \n",
      "                                           aten::linear         0.24%       1.253ms         4.28%      22.653ms      95.181us       0.000us         0.00%      45.682ms     191.941us           0 b           0 b      12.90 Gb      -1.00 Gb           238  \n",
      "                                    aten::empty_strided         0.99%       5.232ms         0.99%       5.232ms       5.560us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      11.64 Gb      11.64 Gb           941  \n",
      "                                               aten::to         0.14%     760.400us         6.94%      36.714ms      54.878us       0.000us         0.00%      19.631ms      29.343us           0 b           0 b       9.39 Gb           0 b           669  \n",
      "                                         aten::_to_copy         0.53%       2.781ms         6.80%      35.953ms      53.822us       0.000us         0.00%      19.631ms      29.387us           0 b           0 b       9.39 Gb           0 b           668  \n",
      "                                            aten::empty         0.31%       1.640ms         0.31%       1.640ms       4.837us       0.000us         0.00%       0.000us       0.000us         228 b         228 b       7.79 Gb       7.79 Gb           339  \n",
      "                                           MulBackward0         0.04%     200.167us         0.98%       5.183ms     110.277us       0.000us         0.00%      16.564ms     352.423us           0 b           0 b       7.38 Gb           0 b            47  \n",
      "                                               aten::mm         1.08%       5.740ms         6.90%      36.529ms     152.841us      37.356ms        15.46%      37.356ms     156.300us           0 b           0 b       7.37 Gb       7.37 Gb           239  \n",
      "                                           DivBackward0         0.07%     355.687us         4.05%      21.427ms     691.201us       0.000us         0.00%      24.737ms     797.968us           0 b           0 b       5.23 Gb      -7.85 Gb            31  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 529.098ms\n",
      "Self CUDA time total: 241.668ms\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd8a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026f0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ab23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f55fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/work/ludwig/jthuemmel54/miniforge3/envs/torch/lib/python3.13/site-packages/torch/profiler/profiler.py:488: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • 100‑run time     : 7.4787 s\n",
      "  • Peak VRAM (100 runs): 6439.1 MiB\n",
      "  • torch.profiler summary:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::mul         1.38%       1.955ms         3.99%       5.643ms      29.544us       4.105ms         5.60%       4.105ms      21.491us           0 b           0 b       1.84 Gb       1.84 Gb           191  \n",
      "                                               aten::mm         3.02%       4.275ms         4.45%       6.296ms      27.255us      43.468ms        59.29%      43.468ms     188.175us           0 b           0 b       1.46 Gb       1.46 Gb           231  \n",
      "                                            aten::empty         1.03%       1.451ms         1.03%       1.451ms       4.805us       0.000us         0.00%       0.000us       0.000us         212 b         212 b       1.36 Gb       1.36 Gb           302  \n",
      "                                              aten::div         1.01%       1.422ms         2.53%       3.574ms      26.673us       2.363ms         3.22%       2.363ms      17.632us           0 b           0 b       1.23 Gb       1.23 Gb           134  \n",
      "                                           aten::linear         0.37%     518.638us         4.77%       6.752ms      60.826us       0.000us         0.00%      16.787ms     151.236us           0 b           0 b       1.08 Gb           0 b           111  \n",
      "                                           aten::matmul         0.12%     171.019us         1.11%       1.571ms      52.359us       0.000us         0.00%      13.208ms     440.265us           0 b           0 b     977.50 Mb           0 b            30  \n",
      "                                           MulBackward0         0.11%     156.080us         1.95%       2.761ms      69.025us       0.000us         0.00%       1.894ms      47.357us           0 b           0 b     848.00 Mb           0 b            40  \n",
      "                                              aten::add         0.93%       1.313ms         1.40%       1.974ms      19.741us       1.318ms         1.80%       1.318ms      13.180us           0 b           0 b     620.62 Mb     620.62 Mb           100  \n",
      "                                            MmBackward0         0.17%     240.448us         1.41%       2.000ms      71.444us       0.000us         0.00%      25.437ms     908.461us           0 b           0 b     607.50 Mb           0 b            28  \n",
      "                                           DivBackward0         0.20%     285.389us         3.26%       4.616ms     177.524us       0.000us         0.00%       2.381ms      91.571us           0 b           0 b     484.00 Mb    -726.00 Mb            26  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 141.415ms\n",
      "Self CUDA time total: 73.311ms\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74596701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/work/ludwig/jthuemmel54/miniforge3/envs/torch/lib/python3.13/site-packages/torch/profiler/profiler.py:488: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • 100‑run time     : 10.4159 s\n",
      "  • Peak VRAM (100 runs): 7261.3 MiB\n",
      "  • torch.profiler summary:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::mul         0.98%       2.128ms         2.31%       5.002ms      24.761us       5.063ms         4.87%       5.063ms      25.064us           0 b           0 b       2.56 Gb       2.56 Gb           202  \n",
      "                                               aten::mm         2.13%       4.607ms         8.21%      17.737ms      71.808us      62.940ms        60.57%      62.940ms     254.816us           0 b           0 b       1.96 Gb       1.96 Gb           247  \n",
      "                                              aten::div         0.70%       1.520ms         2.33%       5.026ms      35.396us       2.960ms         2.85%       2.960ms      20.845us           0 b           0 b       1.66 Gb       1.66 Gb           142  \n",
      "                                            aten::empty         0.71%       1.532ms         0.71%       1.532ms       4.894us       0.000us         0.00%       0.000us       0.000us         212 b         212 b       1.57 Gb       1.57 Gb           313  \n",
      "                                           aten::linear         0.25%     546.274us         3.04%       6.578ms      59.257us       0.000us         0.00%      23.818ms     214.578us           0 b           0 b       1.44 Gb           0 b           111  \n",
      "                                           MulBackward0         0.08%     167.939us         1.16%       2.510ms      58.382us       0.000us         0.00%       2.409ms      56.031us           0 b           0 b       1.18 Gb           0 b            43  \n",
      "                                           aten::matmul         0.07%     156.340us         0.67%       1.450ms      48.328us       0.000us         0.00%      17.319ms     577.305us           0 b           0 b       1.15 Gb           0 b            30  \n",
      "                                              aten::add         0.61%       1.318ms         0.91%       1.968ms      19.677us       1.559ms         1.50%       1.559ms      15.586us           0 b           0 b     819.12 Mb     819.12 Mb           100  \n",
      "                                            MmBackward0         0.12%     254.398us         3.11%       6.726ms     224.193us       0.000us         0.00%      34.437ms       1.148ms           0 b           0 b     743.00 Mb           0 b            30  \n",
      "                                           DivBackward0         0.14%     313.320us         1.79%       3.877ms     138.468us       0.000us         0.00%       2.887ms     103.100us           0 b           0 b     662.00 Mb    -994.00 Mb            28  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 216.125ms\n",
      "Self CUDA time total: 103.921ms\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "031ebb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/work/ludwig/jthuemmel54/miniforge3/envs/torch/lib/python3.13/site-packages/torch/profiler/profiler.py:488: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • 100‑run time     : 18.4837 s\n",
      "  • Peak VRAM (100 runs): 8789.8 MiB\n",
      "  • torch.profiler summary:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::mul         0.55%       2.124ms         5.08%      19.593ms      96.993us       8.241ms         4.45%       8.241ms      40.796us           0 b           0 b       3.98 Gb       3.98 Gb           202  \n",
      "                                               aten::mm         1.23%       4.748ms         7.72%      29.771ms     120.042us     100.793ms        54.41%     100.793ms     406.424us           0 b           0 b       2.98 Gb       2.98 Gb           248  \n",
      "                                              aten::div         0.40%       1.550ms         4.16%      16.038ms     112.943us       4.378ms         2.36%       4.378ms      30.833us           0 b           0 b       2.48 Gb       2.48 Gb           142  \n",
      "                                           aten::linear         0.15%     559.564us         4.51%      17.408ms     156.826us       0.000us         0.00%      38.473ms     346.600us           0 b           0 b       2.20 Gb           0 b           111  \n",
      "                                            aten::empty         0.41%       1.577ms         0.41%       1.577ms       4.974us       0.000us         0.00%       0.000us       0.000us         212 b         212 b       2.05 Gb       2.05 Gb           317  \n",
      "                                           MulBackward0         0.04%     171.340us         2.53%       9.764ms     227.072us       0.000us         0.00%       3.930ms      91.399us           0 b           0 b       1.85 Gb           0 b            43  \n",
      "                                           aten::matmul         0.04%     163.438us         1.50%       5.778ms     186.387us       0.000us         0.00%      26.161ms     843.895us           0 b           0 b       1.60 Gb           0 b            31  \n",
      "                                              aten::add         0.34%       1.305ms         1.94%       7.466ms      74.661us       2.436ms         1.31%       2.436ms      24.356us           0 b           0 b       1.20 Gb       1.20 Gb           100  \n",
      "                                            MmBackward0         0.08%     299.835us         1.83%       7.063ms     227.835us       0.000us         0.00%      52.584ms       1.696ms           0 b           0 b    1019.50 Mb           0 b            31  \n",
      "                                           DivBackward0         0.08%     308.067us         3.79%      14.619ms     522.122us       0.000us         0.00%       4.501ms     160.752us           0 b           0 b    1001.00 Mb      -1.47 Gb            28  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 385.650ms\n",
      "Self CUDA time total: 185.241ms\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "self vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740da17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/work/ludwig/jthuemmel54/miniforge3/envs/torch/lib/python3.13/site-packages/torch/profiler/profiler.py:488: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • 100‑run time     : 8.9925 s\n",
      "  • Peak VRAM (100 runs): 3137.7 MiB\n",
      "  • torch.profiler summary:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::mul         1.19%       2.055ms         1.90%       3.293ms      16.300us       4.957ms         5.65%       4.957ms      24.541us           0 b           0 b       2.22 Gb       2.22 Gb           202  \n",
      "                                               aten::mm         2.60%       4.504ms         4.48%       7.750ms      31.251us      52.759ms        60.09%      52.759ms     212.737us           0 b           0 b       1.74 Gb       1.74 Gb           248  \n",
      "                                            aten::empty         0.88%       1.529ms         0.88%       1.529ms       4.838us       0.000us         0.00%       0.000us       0.000us         212 b         212 b       1.53 Gb       1.53 Gb           316  \n",
      "                                              aten::div         0.86%       1.483ms         1.36%       2.346ms      16.524us       2.761ms         3.14%       2.761ms      19.446us           0 b           0 b       1.46 Gb       1.46 Gb           142  \n",
      "                                           aten::linear         0.31%     536.625us         3.78%       6.546ms      58.977us       0.000us         0.00%      19.472ms     175.423us           0 b           0 b       1.27 Gb           0 b           111  \n",
      "                                           aten::matmul         0.09%     162.985us         0.89%       1.544ms      49.799us       0.000us         0.00%      15.052ms     485.562us           0 b           0 b       1.08 Gb           0 b            31  \n",
      "                                           MulBackward0         0.09%     163.740us         0.90%       1.563ms      36.348us       0.000us         0.00%       2.319ms      53.934us           0 b           0 b       1.02 Gb           0 b            43  \n",
      "                                            MmBackward0         0.17%     287.457us         1.43%       2.471ms      79.723us       0.000us         0.00%      30.463ms     982.678us           0 b           0 b     702.00 Mb           0 b            31  \n",
      "                                              aten::add         0.74%       1.276ms         1.10%       1.913ms      19.319us       1.449ms         1.65%       1.449ms      14.631us           0 b           0 b     681.12 Mb     681.12 Mb            99  \n",
      "                                           DivBackward0         0.16%     283.095us         1.31%       2.260ms      80.730us       0.000us         0.00%       2.834ms     101.225us           0 b           0 b     580.00 Mb    -870.00 Mb            28  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 173.097ms\n",
      "Self CUDA time total: 87.807ms\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ambient src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42dda76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/work/ludwig/jthuemmel54/miniforge3/envs/torch/lib/python3.13/site-packages/torch/profiler/profiler.py:488: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • 100‑run time     : 11.7760 s\n",
      "  • Peak VRAM (100 runs): 3724.0 MiB\n",
      "  • torch.profiler summary:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::mul         1.44%       2.335ms         2.18%       3.534ms      17.497us       5.444ms         4.95%       5.444ms      26.951us           0 b           0 b       2.76 Gb       2.76 Gb           202  \n",
      "                                               aten::mm         2.84%       4.615ms         4.27%       6.931ms      28.060us      66.164ms        60.22%      66.164ms     267.871us           0 b           0 b       2.11 Gb       2.11 Gb           247  \n",
      "                                              aten::div         0.93%       1.515ms         1.46%       2.370ms      16.690us       3.100ms         2.82%       3.100ms      21.833us           0 b           0 b       1.77 Gb       1.77 Gb           142  \n",
      "                                            aten::empty         0.96%       1.550ms         0.96%       1.550ms       4.953us       0.000us         0.00%       0.000us       0.000us         212 b         212 b       1.63 Gb       1.63 Gb           313  \n",
      "                                           aten::linear         0.81%       1.320ms         6.16%      10.002ms      90.108us       0.000us         0.00%      24.767ms     223.124us           0 b           0 b       1.54 Gb           0 b           111  \n",
      "                                           MulBackward0         0.10%     162.087us         0.96%       1.557ms      36.207us       0.000us         0.00%       2.604ms      60.568us           0 b           0 b       1.27 Gb           0 b            43  \n",
      "                                           aten::matmul         0.10%     165.818us         2.18%       3.532ms     117.749us       0.000us         0.00%      17.754ms     591.787us           0 b           0 b       1.21 Gb           0 b            30  \n",
      "                                              aten::add         0.79%       1.289ms         1.19%       1.929ms      19.482us       1.627ms         1.48%       1.627ms      16.431us           0 b           0 b     865.12 Mb     865.12 Mb            99  \n",
      "                                            MmBackward0         0.15%     242.537us         1.36%       2.205ms      73.485us       0.000us         0.00%      35.964ms       1.199ms           0 b           0 b     781.00 Mb           0 b            30  \n",
      "                                           DivBackward0         0.20%     320.155us         1.41%       2.293ms      81.889us       0.000us         0.00%       3.073ms     109.742us           0 b           0 b     708.50 Mb      -1.04 Gb            28  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 162.294ms\n",
      "Self CUDA time total: 109.879ms\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latent src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e4044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0019a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
