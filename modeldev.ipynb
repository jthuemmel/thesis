{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4330e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.nn import Module, Embedding, Linear, ModuleList\n",
    "from einops import rearrange, reduce, repeat\n",
    "from utils.components import TransformerBlock, ConditionalLayerNorm, GatedFFN, Attention, Interface\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "from utils.vit import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c6b881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModalEncoder(Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.in_projection = Embedding(cfg.num_features, cfg.dim * cfg.dim_in)\n",
    "        self.feature_bias = Embedding(cfg.num_features, cfg.dim)\n",
    "        self.queries = Embedding(1, cfg.dim)       \n",
    "        self.kv_norm = ConditionalLayerNorm(cfg.dim, cfg.dim_ctx)\n",
    "        self.cross_attn = TransformerBlock(cfg.dim, dim_ctx=cfg.dim_ctx)\n",
    "\n",
    "    def forward(self, data, idx, ctx = None):\n",
    "        # ensure correct shapes\n",
    "        B, N, _, I = data.size()\n",
    "        # get dynamic weights \n",
    "        w = self.in_projection(idx)\n",
    "        w = rearrange(w, '... f (d i) -> ... f d i', i = I)\n",
    "        b = self.feature_bias(idx)\n",
    "        b = rearrange(b, \"... f d -> ... () f d\")\n",
    "        # linear projection\n",
    "        kv = torch.einsum('b n f i, ... f d i -> b n f d', data, w)\n",
    "        # normalize and add feature-bias\n",
    "        kv = self.kv_norm(kv, ctx) + b\n",
    "        # expand query vectors\n",
    "        q = repeat(self.queries.weight, 'q d -> b n q d', b = B, n = N)\n",
    "        # cross attend\n",
    "        q = self.cross_attn(q = q, kv = kv, ctx = ctx).squeeze(2)\n",
    "        return q\n",
    "    \n",
    "class ModalDecoder(Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.norm = ConditionalLayerNorm(cfg.dim, cfg.dim_ctx)\n",
    "        self.ffn = GatedFFN(cfg.dim)\n",
    "        self.out_projection = Embedding(cfg.num_features, cfg.dim * cfg.dim_out)\n",
    "\n",
    "    def forward(self, latent, idx, ctx = None):\n",
    "        _, _, D = latent.size()\n",
    "        latent = self.ffn(self.norm(latent, ctx))\n",
    "        w = self.out_projection(idx)\n",
    "        w = rearrange(w, \"... f (d o) -> ... f d o\", d = D)\n",
    "        out = torch.einsum(\"b n d, ... f d o -> b n f o\", latent, w)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f27b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ambient_src(self, src, tgt, latents, ctx):\n",
    "    # O(2*z*(src+tgt) + z**2)\n",
    "    x = torch.cat([src, tgt], dim = 1)\n",
    "    for block in self.network:\n",
    "        x, z = block(x = x, z = latents, ctx = ctx)\n",
    "    _, tgt = x.split([src.size(1), tgt.size(1)], dim = 1)\n",
    "    return tgt, z\n",
    "\n",
    "def predict_latent_src(self, src, tgt, latents, ctx):\n",
    "    #O(2*tgt*(src + z) + (src * z)**2)\n",
    "    z = torch.cat([src, latents], dim = 1)\n",
    "    for block in self.network:\n",
    "        tgt, z = block(x = tgt, z = z, ctx = ctx)\n",
    "    _, z = z.split([src.size(1), latents.size(1)], dim=1)\n",
    "    return tgt, z\n",
    "\n",
    "def predict_self_vit(self, src, tgt, latents, ctx):\n",
    "    x = torch.cat([src, latents], dim = 1)\n",
    "    encoder, decoder = self.network[:-1], self.network[-1]\n",
    "    for block in encoder:\n",
    "        x, _ = block(x, x, ctx)\n",
    "    q = torch.cat([x, tgt], dim = 1)\n",
    "    q = decoder(q, q, ctx)\n",
    "    _, tgt = q.split([x.size(1), tgt.size(1)], dim = 1)\n",
    "    return tgt, None\n",
    "\n",
    "def predict_cross_vit(self, src, tgt, latents, ctx):\n",
    "    x = torch.cat([src, latents], dim = 1)\n",
    "    encoder, decoder = self.network[:-1], self.network[-1]\n",
    "    for block in encoder:\n",
    "        x, _ = block(x, x, ctx)\n",
    "    tgt = decoder(tgt, x, ctx)\n",
    "    return tgt, None\n",
    "\n",
    "def predict_perceiver(self, src, tgt, latents, ctx):\n",
    "    encoder, decoder = self.network[:-1], self.network[-1]\n",
    "    for block in encoder:\n",
    "        _, z = block(src, latents, ctx)\n",
    "    tgt = decoder(tgt, z, ctx)\n",
    "    return tgt, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87c1130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedInterface(Module):\n",
    "    def __init__(self, cfg: dataclass):\n",
    "        super().__init__()\n",
    "        \n",
    "        # In/Out\n",
    "        self.proj_in = Linear(cfg.dim_in, cfg.dim)\n",
    "        self.proj_noise = Linear(cfg.dim_ctx, cfg.dim_ctx)\n",
    "        self.dim_ctx = cfg.dim_ctx\n",
    "        self.proj_out = Linear(cfg.dim, cfg.dim_out)\n",
    "\n",
    "        # Embeddings\n",
    "        self.queries = Embedding(1, cfg.dim)\n",
    "        self.latents = Embedding(cfg.num_latents, cfg.dim)\n",
    "        self.positions = Embedding(cfg.num_tokens, cfg.dim)\n",
    "\n",
    "        # Interfaces\n",
    "        self.network = ModuleList([\n",
    "            Interface(cfg.dim, cfg.num_compute_blocks, dim_ctx= cfg.dim_ctx, dim_heads= cfg.dim_heads) for _ in range(cfg.num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, src, src_pos, tgt_pos, ctx = None):  \n",
    "        #initialize context\n",
    "        ctx = self.proj_noise(ctx if ctx is not None else src.new_zeros(self.dim_ctx))\n",
    "        # initialize src\n",
    "        src = self.proj_in(src) + self.positions(src_pos)\n",
    "        # initialize latents\n",
    "        latents = repeat(self.latents.weight, \"z d -> b z d\", b = src.size(0))      \n",
    "        # initialize tgt\n",
    "        tgt = self.queries(torch.zeros_like(tgt_pos)) + self.positions(tgt_pos)\n",
    "        # predict\n",
    "        z = torch.cat([src, latents], dim = 1)\n",
    "        for block in self.network:\n",
    "            tgt, z = block(x = tgt, z = z, ctx = ctx)\n",
    "        _, z = z.split([src.size(1), latents.size(1)], dim=1)\n",
    "        # project out\n",
    "        tgt = self.proj_out(tgt)\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d54a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedViT(Module):\n",
    "    def __init__(self, cfg: dataclass):\n",
    "        super().__init__()\n",
    "        # Embeddings\n",
    "        self.queries = Embedding(1, cfg.dim)\n",
    "        self.network = ViT(cfg)\n",
    "        self.predict = TransformerBlock(cfg.dim, dim_ctx=cfg.dim_ctx)\n",
    "\n",
    "    def forward(self, x, src_pos, tgt_pos, ctx = None):   \n",
    "        src = self.network(x, src_pos)\n",
    "        tgt = self.queries(torch.zeros_like(tgt_pos)) + self.network.positions(tgt_pos)\n",
    "        # predict\n",
    "        kv = torch.cat([src, tgt], dim = 1)\n",
    "        tgt = self.predict(tgt, kv, ctx = ctx)\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "183f0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModalWrapper(Module):\n",
    "    def __init__(self, interface_cfg:dataclass, modal_cfg: dataclass):\n",
    "        super().__init__()\n",
    "        # Networks\n",
    "        self.encoder = ModalEncoder(modal_cfg)\n",
    "        self.processor = MaskedInterface(interface_cfg)\n",
    "        self.decoder = ModalDecoder(modal_cfg)\n",
    "        \n",
    "        # Init\n",
    "        self.apply(self.base_init)\n",
    "        self.apply(self.zero_init)\n",
    "\n",
    "    @staticmethod\n",
    "    def base_init(m):\n",
    "        if isinstance(m, Linear):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = 0.02)\n",
    "        if isinstance(m, Embedding):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = 0.02)\n",
    "\n",
    "    @staticmethod\n",
    "    def zero_init(m):\n",
    "        if isinstance(m, Attention):\n",
    "            torch.nn.init.zeros_(m.to_out.weight)\n",
    "        if isinstance(m, GatedFFN):\n",
    "            torch.nn.init.zeros_(m.to_out.weight)\n",
    "        if isinstance(m, ConditionalLayerNorm):\n",
    "            torch.nn.init.zeros_(m.linear.weight)\n",
    "\n",
    "    def forward(self, data: torch.Tensor, coords: tuple, noise: torch.Tensor):\n",
    "        K = noise.size(0) // data.size(0)\n",
    "        pos_src, pos_tgt, var_src, var_tgt = coords\n",
    "        x = self.encoder(data, var_src, ctx = None)\n",
    "        z = self.processor(x, pos_src, pos_tgt, ctx = None)\n",
    "        z_hat = repeat(z, \"b ... -> (b k) ...\", k = K)\n",
    "        var_tgt = repeat(var_tgt, \"b ... -> (b k) ...\", k = K)\n",
    "        x_hat = self.decoder(z_hat, var_tgt, noise)\n",
    "        x_hat = rearrange(x_hat, \"(b k) ... -> b ... k\", k = K)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2253f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    dim_in: int\n",
    "    dim_out: int\n",
    "    dim: int\n",
    "    num_features: Optional[int] = None\n",
    "    num_tokens: Optional[int] = None\n",
    "    num_compute_blocks: Optional[int] = None\n",
    "    num_layers: Optional[int] = None\n",
    "    num_latents: Optional[int] = None\n",
    "    num_cls: Optional[int] = None\n",
    "    dim_heads: int = 64\n",
    "    dim_ctx: int = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d186098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "44b9914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, N, F, C = 8, 4096, 73, 4\n",
    "D = 512\n",
    "K = 4\n",
    "M_src, M_tgt, F_src, F_tgt = 256, 1024, 32, 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb9f763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modal_cfg = Config(C, C, D, F, N)\n",
    "interface_cfg = Config(D, D, D, None, N, 4, 2, 32)\n",
    "vit_cfg = Config(D, D, D, None, N, None, 12, None, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9278edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_src = torch.multinomial(torch.ones((B, N)), M_src)\n",
    "pos_tgt = torch.multinomial(torch.ones((B, N)), M_tgt)\n",
    "var_src = torch.multinomial(torch.ones((B, F)), F_src)\n",
    "var_tgt = torch.multinomial(torch.ones((B, F)), F_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d30a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((B, M_src, F_src, C))\n",
    "noise = torch.randn((B * 4, 1, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "59eae362",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModalWrapper(interface_cfg, modal_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b42dda76",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model(x, (pos_src, pos_tgt, var_src, var_tgt), noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13d19dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024, 32, 4, 4])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ada75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6451e1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
