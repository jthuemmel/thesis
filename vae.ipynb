{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e36635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "\n",
    "from einops.layers.torch import EinMix, Rearrange\n",
    "\n",
    "from utils.components import *\n",
    "from utils.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5278711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6549798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NetworkConfig(dim=512, num_latents=32, num_compute_blocks=12, num_tails=1, dim_out=32, dim_coords=64)\n",
    "world = WorldConfig(field_sizes={\"v\": 9, \"t\": 36, \"h\": 64, \"w\": 120}, patch_sizes={'vv': 1, 'tt': 6, 'hh': 8, 'ww': 8}, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30930d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EinDecoder(torch.nn.Module):\n",
    "    def __init__(self, network: NetworkConfig, world: WorldConfig):\n",
    "        super().__init__()\n",
    "        # config attributes \n",
    "        c = network.dim_out\n",
    "        k = default(network.num_tails, 1)\n",
    "        vv = world.patch_sizes['vv']\n",
    "        v = world.token_sizes['v']\n",
    "        groups = v * vv * k\n",
    "\n",
    "        # project tokens to low dimensional space before upsampling\n",
    "        self.token_to_grid = EinMix(\n",
    "            pattern=f\"b {world.flat_token_pattern} d -> b (v vv k c) t h w\",\n",
    "            weight_shape=f\"v vv k c d\",\n",
    "            d = network.dim, c = c, k = k, vv = vv,\n",
    "            **world.token_sizes\n",
    "        )\n",
    "\n",
    "        # small CNN for post-processing\n",
    "        self.cnn = torch.nn.ModuleList([\n",
    "            ConvNextBlock(c * groups, (3, 7, 7), groups) \n",
    "            for _ in range(default(network.num_cnn_blocks, 0))\n",
    "            ])\n",
    "\n",
    "        # upsample grid via interpolate + depthwise separable convolution\n",
    "        self.upsample = ConvInterpolate(c * groups, (3, 5, 5), \n",
    "                                        num_groups= c * groups,\n",
    "                                        out_size= tuple(world.field_sizes[ax] for ax in ['t', 'h', 'w']),\n",
    "                                        mode='area')\n",
    "        \n",
    "        # pointwise projection to output\n",
    "        self.grid_to_field = EinMix(\n",
    "            f'b (v vv k c) (t tt) (h hh) (w ww) -> b {world.field_pattern} k',\n",
    "            weight_shape=\"v vv k c\",\n",
    "            k = k, c = c, **world.patch_sizes, **world.token_sizes\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor):\n",
    "        x = self.token_to_grid(x)\n",
    "        for block in self.cnn:\n",
    "            x = block(x)\n",
    "        x = self.upsample(x)\n",
    "        x = self.grid_to_field(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6052645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "\n",
    "from einops.layers.torch import EinMix\n",
    "\n",
    "from utils.components import *\n",
    "from utils.config import *\n",
    "from utils.random_fields import RandomField\n",
    "\n",
    "class EinMask(torch.nn.Module):\n",
    "    def __init__(self, network: NetworkConfig, world: WorldConfig):\n",
    "        super().__init__()\n",
    "        # store configs\n",
    "        self.network = network\n",
    "        self.world = world\n",
    "\n",
    "        # I/O\n",
    "        self.to_tokens = EinMix(\n",
    "            pattern=f\"b {world.field_pattern} -> b {world.flat_token_pattern} d\", \n",
    "            weight_shape=f'{world.patch_pattern} v d', \n",
    "            d = network.dim, \n",
    "            **world.patch_sizes, **world.token_sizes\n",
    "            )\n",
    "        \n",
    "        self.to_fields = EinDecoder(network, world)\n",
    "                \n",
    "        # noise mapping\n",
    "        if default(network.num_tails, 1) > 1:\n",
    "            self.noise_generator = None\n",
    "        else:\n",
    "            self.noise_generator = RandomField(network.dim, world, has_ffn=False)\n",
    "        \n",
    "        # positional embeddings\n",
    "        self.src_positions = ContinuousPositionalEmbedding(\n",
    "            dim_per_coord=network.dim_coords, \n",
    "            wavelengths=[(1, 2 * k) for k in world.token_shape],\n",
    "            model_dim=network.dim\n",
    "        )\n",
    "\n",
    "        self.tgt_positions = ContinuousPositionalEmbedding(\n",
    "            dim_per_coord=network.dim_coords, \n",
    "            wavelengths=[(1, 2 * k) for k in world.token_shape],\n",
    "            model_dim=network.dim\n",
    "        )\n",
    "        \n",
    "        # pre-computed coordinates\n",
    "        self.register_buffer('indices', torch.arange(world.num_tokens))\n",
    "        self.register_buffer(\"coordinates\", torch.stack(\n",
    "            torch.unravel_index(indices = self.indices, shape = world.token_shape), \n",
    "            dim = -1)\n",
    "            )        \n",
    "        \n",
    "        # learnable latents\n",
    "        self.latents = torch.nn.Embedding(network.num_latents, network.dim)\n",
    "\n",
    "        # latent transformer\n",
    "        self.encoder = torch.nn.ModuleList([\n",
    "            TransformerBlock(network.dim, dim_ctx=network.dim_noise)\n",
    "            for _ in range(default(network.num_read_blocks, 1))\n",
    "        ])\n",
    "\n",
    "        self.processor = torch.nn.ModuleList([\n",
    "            TransformerBlock(network.dim, drop_path=network.drop_path, dim_ctx=network.dim_noise)\n",
    "            for _ in range(default(network.num_compute_blocks, 1))\n",
    "        ])\n",
    "        \n",
    "        self.decoder = torch.nn.ModuleList([\n",
    "            TransformerBlock(network.dim, dim_ctx=network.dim_noise)\n",
    "            for n in range(default(network.num_write_blocks, 1))\n",
    "        ])\n",
    "\n",
    "        # Weight initialization\n",
    "        self.apply(self.base_init)\n",
    "        self.apply(self.zero_init)\n",
    "\n",
    "    @staticmethod\n",
    "    def zero_init(m: torch.nn.Module):\n",
    "        # residual blocks zero out their last layer \n",
    "        if isinstance(m, (TransformerBlock, ConvNextBlock, ConvInterpolate)):\n",
    "            for name, sm in m.named_modules():\n",
    "                if \"_out\" in name and hasattr(sm, 'weight'):\n",
    "                    torch.nn.init.trunc_normal_(sm.weight, std = 1e-7)\n",
    "\n",
    "    @staticmethod\n",
    "    def base_init(m: torch.nn.Module):\n",
    "        # linear\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = 0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        # embedding\n",
    "        elif isinstance(m, torch.nn.Embedding):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = 0.02)\n",
    "        # einmix\n",
    "        elif isinstance(m, EinMix):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = 0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.trunc_normal_(m.bias, std = 0.02)\n",
    "        # convolution\n",
    "        elif isinstance(m, torch.nn.Conv3d):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = 0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        # conditional layer norm\n",
    "        elif isinstance(m, ConditionalLayerNorm):\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "            if m.weight is not None: # CLN weight close to 0\n",
    "                torch.nn.init.trunc_normal_(m.weight, std = 1e-7)\n",
    "    \n",
    "    def forward(self, \n",
    "                fields: torch.FloatTensor, \n",
    "                srcs: List[torch.LongTensor] | torch.LongTensor, \n",
    "                tgts: List[torch.LongTensor] | torch.LongTensor = None,\n",
    "                members: Optional[int] = None, \n",
    "                rng: Optional[torch.Generator] = None\n",
    "                ) -> torch.FloatTensor:\n",
    "        B = fields.size(0)\n",
    "        D = self.network.dim\n",
    "        K = default(self.network.num_tails, 1)\n",
    "        E = default(members, 1)\n",
    "\n",
    "        # expand to ensemble form\n",
    "        fields = einops.repeat(fields, \"b ... -> (b e) ...\", e = E, b = B)\n",
    "        latents = einops.repeat(self.latents.weight, '... -> (b e) ...', b = B, e = E)\n",
    "        coo = einops.repeat(self.coordinates, '... -> (b e) ...', b = B, e = E)\n",
    "        src_idx = einops.repeat(srcs, 'b ... -> (b e) ... d', d = D, e = E, b = B)\n",
    "        \n",
    "        # embed full fields as tokens\n",
    "        tokens = self.to_tokens(fields).gather(1, src_idx)\n",
    "\n",
    "        # prepare tgt and src\n",
    "        tgt = self.tgt_positions(coo)\n",
    "        src = self.src_positions(coo).scatter_add_(1, src_idx, tokens)\n",
    "\n",
    "        # maybe add random field\n",
    "        if exists(self.noise_generator):\n",
    "            noise = self.noise_generator(shape = (B * E,), rng = rng).to(src.dtype)\n",
    "            tgt = tgt + noise\n",
    "            src = src + noise\n",
    "\n",
    "        # map src to latents\n",
    "        for read in self.encoder:\n",
    "            latents = read(q = latents, kv = torch.cat([src, latents], dim = 1))\n",
    "\n",
    "        # process latents\n",
    "        for process in self.processor:\n",
    "            latents = process(q = latents)\n",
    "\n",
    "        # map latents to tgt\n",
    "        for write in self.decoder:\n",
    "            tgt = write(q = tgt, kv = latents)\n",
    "\n",
    "        # map all tokens back to fields\n",
    "        fields = self.to_fields(tgt)\n",
    "        \n",
    "        # rearrange to ensemble form\n",
    "        fields = einops.rearrange(fields, \"(b e) ... k -> b ... (e k)\", e = E, b = B, k = K)\n",
    "        return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03e18fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=-9.22337e+18, max=9.22337e+18, dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.iinfo(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d36fc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "288/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4d7f645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2548039680"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod([16, 288, 72, 64, 120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "840f27c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pw_out\n",
      "conv_out\n",
      "att.to_out\n",
      "ffn.to_out\n",
      "att.to_out\n",
      "ffn.to_out\n",
      "att.to_out\n",
      "ffn.to_out\n",
      "att.to_out\n",
      "ffn.to_out\n",
      "att.to_out\n",
      "ffn.to_out\n",
      "att.to_out\n",
      "ffn.to_out\n",
      "att.to_out\n",
      "ffn.to_out\n",
      "att.to_out\n",
      "ffn.to_out\n",
      "att.to_out\n",
      "ffn.to_out\n",
      "att.to_out\n",
      "ffn.to_out\n",
      "att.to_out\n",
      "ffn.to_out\n",
      "att.to_out\n",
      "ffn.to_out\n",
      "att.to_out\n",
      "ffn.to_out\n",
      "att.to_out\n",
      "ffn.to_out\n"
     ]
    }
   ],
   "source": [
    "model = EinMask(network=network, world=world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "119f17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(world.batch_size, *world.field_shape)\n",
    "m = torch.multinomial(torch.ones(world.batch_size, world.num_tokens), 512)\n",
    "\n",
    "y = model(x, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bcf753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa1951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
