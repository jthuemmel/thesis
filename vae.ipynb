{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "55e36635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "\n",
    "from einops.layers.torch import EinMix, Rearrange\n",
    "\n",
    "from utils.components import *\n",
    "from utils.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4e1a5a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = (1, 3, 3)\n",
    "4 > len(ks) > 0\n",
    "\n",
    "k = len(ks)\n",
    "assert 4 > k > 0, 'kernel must be 1, 2 or 3d'\n",
    "conv_fn = getattr(torch.nn, f\"Conv{k}d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6549798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NetworkConfig(dim=512, num_latents=32, num_layers=12, num_tails=1, dim_out=32, dim_coords=64)\n",
    "world = WorldConfig(field_sizes={\"v\": 9, \"t\": 36, \"h\": 64, \"w\": 120}, patch_sizes={'vv': 1, 'tt': 6, 'hh': 8, 'ww': 8}, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30930d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNextBlock(torch.nn.Module):\n",
    "    def __init__(self, dim: int, kernel_size: tuple, num_groups: int = 1, expansion_factor: int = 4):\n",
    "        super().__init__()\n",
    "        k = len(kernel_size)\n",
    "        assert 4 > k > 0, 'kernel must be 1, 2 or 3d'\n",
    "        conv_fn = getattr(torch.nn, f\"Conv{k}d\")\n",
    "\n",
    "        self.block = torch.nn.Sequential()\n",
    "\n",
    "        self.block.add_module(\n",
    "            'norm',\n",
    "            torch.nn.GroupNorm(num_groups= num_groups, num_channels= dim)\n",
    "        )\n",
    "\n",
    "        self.block.add_module(\n",
    "            'dw',\n",
    "            conv_fn(\n",
    "                in_channels = dim,\n",
    "                out_channels = dim,\n",
    "                padding = 'same',\n",
    "                kernel_size = kernel_size,\n",
    "                groups = dim,\n",
    "            ))\n",
    "\n",
    "        self.block.add_module(\n",
    "            'pw_in',\n",
    "            conv_fn(\n",
    "                in_channels = dim,\n",
    "                out_channels = dim * expansion_factor,\n",
    "                kernel_size = 1,\n",
    "                groups = num_groups,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.block.add_module('silu',torch.nn.SiLU())\n",
    "\n",
    "        self.block.add_module(\n",
    "            'pw_out',\n",
    "            conv_fn(\n",
    "                in_channels = dim * expansion_factor,\n",
    "                out_channels = dim,\n",
    "                kernel_size = 1,\n",
    "                groups = num_groups,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor):\n",
    "        return self.block(x) + x\n",
    "    \n",
    "class ConvInterpolate(torch.nn.Module):\n",
    "    def __init__(self, dim: int, kernel_size: tuple, out_size: tuple, mode: str = \"nearest-exact\"):\n",
    "        super().__init__()\n",
    "        self._out_size = out_size\n",
    "        self._mode = mode\n",
    "        k = len(kernel_size)\n",
    "        assert 4 > k > 0, 'kernel must be 1, 2 or 3d'\n",
    "        conv_fn = getattr(torch.nn, f\"Conv{k}d\")\n",
    "        self.conv = conv_fn(\n",
    "                in_channels = dim,\n",
    "                out_channels = dim,\n",
    "                padding = 'same',\n",
    "                kernel_size = kernel_size,\n",
    "                groups = dim,\n",
    "            )\n",
    "            \n",
    "    def forward(self, x: torch.FloatTensor):\n",
    "        x = torch.nn.functional.interpolate(x, size = self._out_size, mode = self._mode)\n",
    "        x = x + self.conv(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EinDecoder(torch.nn.Module):\n",
    "    def __init__(self, network: NetworkConfig, world: WorldConfig, depth: int = 0):\n",
    "        super().__init__()\n",
    "        # config attributes \n",
    "        c = network.dim_out\n",
    "        k = default(network.num_tails, 1)\n",
    "        vv = world.patch_sizes['vv']\n",
    "        v = world.token_sizes['v']\n",
    "        groups = v * vv * k\n",
    "\n",
    "        # project tokens to low dimensional space before interpolation\n",
    "        self.token_to_grid = EinMix(\n",
    "            pattern=f\"b {world.flat_token_pattern} d -> b (v vv k c) t h w\",\n",
    "            weight_shape=f\"v vv k c d\",\n",
    "            d = network.dim, c = c, k = k, vv = vv,\n",
    "            **world.token_sizes\n",
    "        )\n",
    "\n",
    "        # small CNN for post-processing\n",
    "        if depth >= 1:\n",
    "            self.cnn = torch.nn.Sequential(*[ConvNextBlock(c * groups, (3, 7, 7), groups) for _ in range(depth)])  \n",
    "        else:\n",
    "            self.cnn = None\n",
    "\n",
    "        # unmix grid via interpolate + depthwise separable convolution\n",
    "        self.upsample = ConvInterpolate(c * groups, (3, 5, 5), out_size= tuple(world.field_sizes[ax] for ax in ['t', 'h', 'w']), mode='nearest-exact')\n",
    "        \n",
    "        self.grid_to_field = EinMix(\n",
    "            f'b (v vv k c) (t tt) (h hh) (w ww) -> b {world.field_pattern} k',\n",
    "            weight_shape=\"v vv k c\",\n",
    "            k = k, c = c, **world.patch_sizes, **world.token_sizes\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor):\n",
    "        x = self.token_to_grid(x)\n",
    "        if exists(self.cnn):\n",
    "            x = self.cnn(x)\n",
    "        x = self.upsample(x)\n",
    "        x = self.grid_to_field(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6052645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a454728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "\n",
    "from einops.layers.torch import EinMix\n",
    "\n",
    "from utils.components import *\n",
    "from utils.config import *\n",
    "from utils.random_fields import RandomField\n",
    "\n",
    "class EinMask(torch.nn.Module):\n",
    "    def __init__(self, network: NetworkConfig, world: WorldConfig):\n",
    "        super().__init__()\n",
    "        # store configs\n",
    "        self.network = network\n",
    "        self.world = world\n",
    "\n",
    "        # I/O\n",
    "        self.to_tokens = EinMix(\n",
    "            pattern=f\"b {world.field_pattern} -> b {world.flat_token_pattern} d\", \n",
    "            weight_shape=f'{world.patch_pattern} v d', \n",
    "            d = network.dim, \n",
    "            **world.patch_sizes, **world.token_sizes\n",
    "            )\n",
    "        \n",
    "        self.to_fields = EinDecoder(network, world)\n",
    "                \n",
    "        # noise mapping\n",
    "        if default(network.num_tails, 1) > 1:\n",
    "            self.noise_generator = None\n",
    "        else:\n",
    "            self.noise_generator = RandomField(network.dim, world, has_ffn=False)\n",
    "        \n",
    "        # positional embeddings\n",
    "        self.src_positions = ContinuousPositionalEmbedding(\n",
    "            dim_per_coord=network.dim_coords, \n",
    "            wavelengths=[(1, 2 * k) for k in world.token_shape],\n",
    "            model_dim=network.dim\n",
    "        )\n",
    "\n",
    "        self.tgt_positions = ContinuousPositionalEmbedding(\n",
    "            dim_per_coord=network.dim_coords, \n",
    "            wavelengths=[(1, 2 * k) for k in world.token_shape],\n",
    "            model_dim=network.dim\n",
    "        )\n",
    "        \n",
    "        # pre-computed coordinates\n",
    "        self.register_buffer('indices', torch.arange(world.num_tokens))\n",
    "        self.register_buffer(\"coordinates\", torch.stack(\n",
    "            torch.unravel_index(indices = self.indices, shape = world.token_shape), \n",
    "            dim = -1)\n",
    "            )        \n",
    "        \n",
    "        # learnable latents\n",
    "        self.latents = torch.nn.Embedding(network.num_latents, network.dim)\n",
    "\n",
    "        # latent transformer\n",
    "        self.encoder = torch.nn.ModuleList([\n",
    "            TransformerBlock(network.dim, dim_ctx=network.dim_noise)\n",
    "            for _ in range(default(network.num_read_blocks, 1))\n",
    "        ])\n",
    "\n",
    "        self.processor = torch.nn.ModuleList([\n",
    "            TransformerBlock(network.dim, drop_path=network.drop_path, dim_ctx=network.dim_noise)\n",
    "            for _ in range(default(network.num_compute_blocks, 1))\n",
    "        ])\n",
    "        \n",
    "        self.decoder = torch.nn.ModuleList([\n",
    "            TransformerBlock(network.dim, dim_ctx=network.dim_noise, has_skip= n > 0)\n",
    "            for n in range(default(network.num_write_blocks, 1))\n",
    "        ])\n",
    "\n",
    "        # Weight initialization\n",
    "        self.apply(self.base_init)\n",
    "\n",
    "    @staticmethod\n",
    "    def base_init(m: torch.nn.Module):\n",
    "        # linear\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = 0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        # embedding\n",
    "        if isinstance(m, torch.nn.Embedding):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = 0.02)\n",
    "        # einmix\n",
    "        if isinstance(m, EinMix):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std = 0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.trunc_normal_(m.bias, std = 0.02)\n",
    "        # convolution\n",
    "        if isinstance(m, torch.nn.Conv3d):\n",
    "            torch.nn.init.zeros_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        # conditional layer norm\n",
    "        if isinstance(m, ConditionalLayerNorm):\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "            if m.weight is not None: # CLN weight close to 0\n",
    "                torch.nn.init.trunc_normal_(m.weight, std = 1e-7)\n",
    "    \n",
    "    def forward(self, \n",
    "                fields: torch.FloatTensor, \n",
    "                srcs: List[torch.LongTensor] | torch.LongTensor, \n",
    "                tgts: List[torch.LongTensor] | torch.LongTensor = None,\n",
    "                members: Optional[int] = None, \n",
    "                rng: Optional[torch.Generator] = None\n",
    "                ) -> torch.FloatTensor:\n",
    "        B = fields.size(0)\n",
    "        D = self.network.dim\n",
    "        K = default(self.network.num_tails, 1)\n",
    "        E = default(members, 1)\n",
    "\n",
    "        # expand to ensemble form\n",
    "        fields = einops.repeat(fields, \"b ... -> (b e) ...\", e = E, b = B)\n",
    "        latents = einops.repeat(self.latents.weight, '... -> (b e) ...', b = B, e = E)\n",
    "        coo = einops.repeat(self.coordinates, '... -> (b e) ...', b = B, e = E)\n",
    "        src_idx = einops.repeat(srcs, 'b ... -> (b e) ... d', d = D, e = E, b = B)\n",
    "        \n",
    "        # embed full fields as tokens\n",
    "        tokens = self.to_tokens(fields).gather(1, src_idx)\n",
    "\n",
    "        # prepare tgt and src\n",
    "        tgt = self.tgt_positions(coo)\n",
    "        src = self.src_positions(coo).scatter_add_(1, src_idx, tokens)\n",
    "\n",
    "        # maybe add random field\n",
    "        if exists(self.noise_generator):\n",
    "            noise = self.noise_generator(shape = (B * E,), rng = rng).to(src.dtype)\n",
    "            tgt = tgt + noise\n",
    "            src = src + noise\n",
    "\n",
    "        # map ctx to latents\n",
    "        for read in self.encoder:\n",
    "            latents = read(q = latents, kv = torch.cat([src, latents], dim = 1))\n",
    "\n",
    "        # process latents\n",
    "        for process in self.processor:\n",
    "            latents = process(q = latents)\n",
    "\n",
    "        # map latents to tgt\n",
    "        for write in self.decoder:\n",
    "            tgt = write(q = tgt, kv = latents)\n",
    "\n",
    "        # map all tokens back to fields\n",
    "        fields = self.to_fields(tgt)\n",
    "        \n",
    "        # rearrange to ensemble form\n",
    "        fields = einops.rearrange(fields, \"(b e) ... k -> b ... (e k)\", e = E, b = B, k = K)\n",
    "        return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "840f27c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EinMask(network=network, world=world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "119f17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(world.batch_size, *world.field_shape)\n",
    "m = torch.multinomial(torch.ones(world.batch_size, world.num_tokens), 512)\n",
    "\n",
    "y = model(x, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bcf753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa1951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
